{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d73fcc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total resumes: 220\n",
      "Sample text:\n",
      " Abhishek Jha\n",
      "Application Development Associate - Accenture\n",
      "\n",
      "Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a\n",
      "\n",
      "â€¢ To work for an organization which provides me the opportunity to improve my skills\n",
      "and knowledge for my individual and company's growth in best possibl\n",
      "Sample annotations:\n",
      " [{'label': ['Skills'], 'points': [{'start': 1295, 'end': 1621, 'text': '\\nâ€¢ Programming language: C, C++, Java\\nâ€¢ Oracle PeopleSoft\\nâ€¢ Internet Of Things\\nâ€¢ Machine Learning\\nâ€¢ Database Management System\\nâ€¢ Computer Networks\\nâ€¢ Operating System worked on: Linux, Windows, Mac\\n\\nNon - Technical Skills\\n\\nâ€¢ Honest and Hard-Working\\nâ€¢ Tolerant and Flexible to Different Situations\\nâ€¢ Polite and Calm\\nâ€¢ Team-Player'}]}, {'label': ['Skills'], 'points': [{'start': 993, 'end': 1153, 'text': 'C (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year),\\nDatabase Management System (Less than 1 year), Java (Less than 1 year)'}]}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Step 1: Load the newline-delimited JSON file\n",
    "data = []\n",
    "with open(\"Entity Recognition in Resumes.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "print(\"Total resumes:\", len(data))\n",
    "print(\"Sample text:\\n\", data[0][\"content\"][:300])\n",
    "print(\"Sample annotations:\\n\", data[0].get(\"annotation\", [])[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb91ccd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           -> (0, 0)\n",
      "A               -> (0, 1)\n",
      "##b             -> (1, 2)\n",
      "##his           -> (2, 5)\n",
      "##he            -> (5, 7)\n",
      "##k             -> (7, 8)\n",
      "J               -> (9, 10)\n",
      "##ha            -> (10, 12)\n",
      "Application     -> (13, 24)\n",
      "Development     -> (25, 36)\n",
      "Associate       -> (37, 46)\n",
      "-               -> (47, 48)\n",
      "A               -> (49, 50)\n",
      "##cc            -> (50, 52)\n",
      "##ent           -> (52, 55)\n",
      "##ure           -> (55, 58)\n",
      "Bengal          -> (60, 66)\n",
      "##uru           -> (66, 69)\n",
      ",               -> (69, 70)\n",
      "Karnataka       -> (71, 80)\n",
      "-               -> (81, 82)\n",
      "Em              -> (83, 85)\n",
      "##ail           -> (85, 88)\n",
      "me              -> (89, 91)\n",
      "on              -> (92, 94)\n",
      "Indeed          -> (95, 101)\n",
      ":               -> (101, 102)\n",
      "indeed          -> (103, 109)\n",
      ".               -> (109, 110)\n",
      "com             -> (110, 113)\n",
      "/               -> (113, 114)\n",
      "r               -> (114, 115)\n",
      "/               -> (115, 116)\n",
      "A               -> (116, 117)\n",
      "##b             -> (117, 118)\n",
      "##his           -> (118, 121)\n",
      "##he            -> (121, 123)\n",
      "##k             -> (123, 124)\n",
      "-               -> (124, 125)\n",
      "J               -> (125, 126)\n",
      "##ha            -> (126, 128)\n",
      "/               -> (128, 129)\n",
      "10              -> (129, 131)\n",
      "##e             -> (131, 132)\n",
      "##7             -> (132, 133)\n",
      "##a             -> (133, 134)\n",
      "##8             -> (134, 135)\n",
      "##c             -> (135, 136)\n",
      "##b             -> (136, 137)\n",
      "##7             -> (137, 138)\n",
      "##32            -> (138, 140)\n",
      "##b             -> (140, 141)\n",
      "##c             -> (141, 142)\n",
      "##43            -> (142, 144)\n",
      "##a             -> (144, 145)\n",
      "â€¢               -> (147, 148)\n",
      "To              -> (149, 151)\n",
      "work            -> (152, 156)\n",
      "for             -> (157, 160)\n",
      "an              -> (161, 163)\n",
      "organization    -> (164, 176)\n",
      "which           -> (177, 182)\n",
      "provides        -> (183, 191)\n",
      "me              -> (192, 194)\n",
      "the             -> (195, 198)\n",
      "opportunity     -> (199, 210)\n",
      "to              -> (211, 213)\n",
      "improve         -> (214, 221)\n",
      "my              -> (222, 224)\n",
      "skills          -> (225, 231)\n",
      "and             -> (232, 235)\n",
      "knowledge       -> (236, 245)\n",
      "for             -> (246, 249)\n",
      "my              -> (250, 252)\n",
      "individual      -> (253, 263)\n",
      "and             -> (264, 267)\n",
      "company         -> (268, 275)\n",
      "'               -> (275, 276)\n",
      "s               -> (276, 277)\n",
      "growth          -> (278, 284)\n",
      "in              -> (285, 287)\n",
      "best            -> (288, 292)\n",
      "possible        -> (293, 301)\n",
      "ways            -> (302, 306)\n",
      ".               -> (306, 307)\n",
      "Will            -> (309, 313)\n",
      "##ing           -> (313, 316)\n",
      "to              -> (317, 319)\n",
      "relocate        -> (320, 328)\n",
      "to              -> (329, 331)\n",
      ":               -> (331, 332)\n",
      "Bangalore       -> (333, 342)\n",
      ",               -> (342, 343)\n",
      "Karnataka       -> (344, 353)\n",
      "W               -> (355, 356)\n",
      "##OR            -> (356, 358)\n",
      "##K             -> (358, 359)\n",
      "E               -> (360, 361)\n",
      "##X             -> (361, 362)\n",
      "##P             -> (362, 363)\n",
      "##ER            -> (363, 365)\n",
      "##IE            -> (365, 367)\n",
      "##NC            -> (367, 369)\n",
      "##E             -> (369, 370)\n",
      "Application     -> (372, 383)\n",
      "Development     -> (384, 395)\n",
      "Associate       -> (396, 405)\n",
      "A               -> (407, 408)\n",
      "##cc            -> (408, 410)\n",
      "##ent           -> (410, 413)\n",
      "##ure           -> (413, 416)\n",
      "-               -> (417, 418)\n",
      "November        -> (420, 428)\n",
      "2017            -> (429, 433)\n",
      "to              -> (434, 436)\n",
      "Present         -> (437, 444)\n",
      "Role            -> (446, 450)\n",
      ":               -> (450, 451)\n",
      "Currently       -> (452, 461)\n",
      "working         -> (462, 469)\n",
      "on              -> (470, 472)\n",
      "Cha             -> (473, 476)\n",
      "##t             -> (476, 477)\n",
      "-               -> (477, 478)\n",
      "b               -> (478, 479)\n",
      "##ot            -> (479, 481)\n",
      ".               -> (481, 482)\n",
      "Dev             -> (483, 486)\n",
      "##elo           -> (486, 489)\n",
      "##ping          -> (489, 493)\n",
      "Back            -> (494, 498)\n",
      "##end           -> (498, 501)\n",
      "Oracle          -> (502, 508)\n",
      "People          -> (509, 515)\n",
      "##S             -> (515, 516)\n",
      "##oft           -> (516, 519)\n",
      "Que             -> (520, 523)\n",
      "##ries          -> (523, 527)\n",
      "for             -> (528, 531)\n",
      "the             -> (532, 535)\n",
      "Bo              -> (536, 538)\n",
      "##t             -> (538, 539)\n",
      "which           -> (540, 545)\n",
      "will            -> (546, 550)\n",
      "be              -> (551, 553)\n",
      "triggered       -> (554, 563)\n",
      "based           -> (564, 569)\n",
      "on              -> (570, 572)\n",
      "given           -> (573, 578)\n",
      "input           -> (579, 584)\n",
      ".               -> (584, 585)\n",
      "Also            -> (586, 590)\n",
      ",               -> (590, 591)\n",
      "Training        -> (592, 600)\n",
      "the             -> (601, 604)\n",
      "b               -> (605, 606)\n",
      "##ot            -> (606, 608)\n",
      "for             -> (609, 612)\n",
      "different       -> (613, 622)\n",
      "possible        -> (623, 631)\n",
      "utter           -> (632, 637)\n",
      "##ance          -> (637, 641)\n",
      "##s             -> (641, 642)\n",
      "(               -> (643, 644)\n",
      "Both            -> (644, 648)\n",
      "positive        -> (649, 657)\n",
      "and             -> (658, 661)\n",
      "negative        -> (662, 670)\n",
      ")               -> (670, 671)\n",
      ",               -> (671, 672)\n",
      "which           -> (673, 678)\n",
      "will            -> (679, 683)\n",
      "be              -> (684, 686)\n",
      "given           -> (687, 692)\n",
      "as              -> (693, 695)\n",
      "input           -> (696, 701)\n",
      "by              -> (702, 704)\n",
      "the             -> (705, 708)\n",
      "user            -> (709, 713)\n",
      ".               -> (713, 714)\n",
      "E               -> (716, 717)\n",
      "##D             -> (717, 718)\n",
      "##UC            -> (718, 720)\n",
      "##AT            -> (720, 722)\n",
      "##ION           -> (722, 725)\n",
      "B               -> (727, 728)\n",
      ".               -> (728, 729)\n",
      "E               -> (729, 730)\n",
      "in              -> (731, 733)\n",
      "Information     -> (734, 745)\n",
      "science         -> (746, 753)\n",
      "and             -> (754, 757)\n",
      "engineering     -> (758, 769)\n",
      "B               -> (771, 772)\n",
      ".               -> (772, 773)\n",
      "v               -> (773, 774)\n",
      ".               -> (774, 775)\n",
      "b               -> (775, 776)\n",
      "college         -> (777, 784)\n",
      "of              -> (785, 787)\n",
      "engineering     -> (788, 799)\n",
      "and             -> (800, 803)\n",
      "technology      -> (804, 814)\n",
      "-               -> (815, 816)\n",
      "Hu              -> (818, 820)\n",
      "##b             -> (820, 821)\n",
      "##li            -> (821, 823)\n",
      ",               -> (823, 824)\n",
      "Karnataka       -> (825, 834)\n",
      "August          -> (836, 842)\n",
      "2013            -> (843, 847)\n",
      "to              -> (848, 850)\n",
      "June            -> (851, 855)\n",
      "2017            -> (856, 860)\n",
      "12th            -> (862, 866)\n",
      "in              -> (867, 869)\n",
      "Mathematics     -> (870, 881)\n",
      "Wood            -> (883, 887)\n",
      "##bine          -> (887, 891)\n",
      "modern          -> (892, 898)\n",
      "school          -> (899, 905)\n",
      "April           -> (907, 912)\n",
      "2011            -> (913, 917)\n",
      "to              -> (918, 920)\n",
      "March           -> (921, 926)\n",
      "2013            -> (927, 931)\n",
      "10th            -> (933, 937)\n",
      "Ken             -> (939, 942)\n",
      "##dr            -> (942, 944)\n",
      "##iya           -> (944, 947)\n",
      "V               -> (948, 949)\n",
      "##idy           -> (949, 952)\n",
      "##alaya         -> (952, 957)\n",
      "April           -> (959, 964)\n",
      "2001            -> (965, 969)\n",
      "to              -> (970, 972)\n",
      "March           -> (973, 978)\n",
      "2011            -> (979, 983)\n",
      "SK              -> (985, 987)\n",
      "##IL            -> (987, 989)\n",
      "##LS            -> (989, 991)\n",
      "C               -> (993, 994)\n",
      "(               -> (995, 996)\n",
      "Less            -> (996, 1000)\n",
      "than            -> (1001, 1005)\n",
      "1               -> (1006, 1007)\n",
      "year            -> (1008, 1012)\n",
      ")               -> (1012, 1013)\n",
      ",               -> (1013, 1014)\n",
      "Database        -> (1015, 1023)\n",
      "(               -> (1024, 1025)\n",
      "Less            -> (1025, 1029)\n",
      "than            -> (1030, 1034)\n",
      "1               -> (1035, 1036)\n",
      "year            -> (1037, 1041)\n",
      ")               -> (1041, 1042)\n",
      ",               -> (1042, 1043)\n",
      "Database        -> (1044, 1052)\n",
      "Management      -> (1053, 1063)\n",
      "(               -> (1064, 1065)\n",
      "Less            -> (1065, 1069)\n",
      "than            -> (1070, 1074)\n",
      "1               -> (1075, 1076)\n",
      "year            -> (1077, 1081)\n",
      ")               -> (1081, 1082)\n",
      ",               -> (1082, 1083)\n",
      "Database        -> (1084, 1092)\n",
      "Management      -> (1093, 1103)\n",
      "System          -> (1104, 1110)\n",
      "(               -> (1111, 1112)\n",
      "Less            -> (1112, 1116)\n",
      "than            -> (1117, 1121)\n",
      "1               -> (1122, 1123)\n",
      "year            -> (1124, 1128)\n",
      ")               -> (1128, 1129)\n",
      ",               -> (1129, 1130)\n",
      "Java            -> (1131, 1135)\n",
      "(               -> (1136, 1137)\n",
      "Less            -> (1137, 1141)\n",
      "than            -> (1142, 1146)\n",
      "1               -> (1147, 1148)\n",
      "year            -> (1149, 1153)\n",
      ")               -> (1153, 1154)\n",
      "AD              -> (1156, 1158)\n",
      "##DI            -> (1158, 1160)\n",
      "##TI            -> (1160, 1162)\n",
      "##ON            -> (1162, 1164)\n",
      "##AL            -> (1164, 1166)\n",
      "IN              -> (1167, 1169)\n",
      "##F             -> (1169, 1170)\n",
      "##OR            -> (1170, 1172)\n",
      "##MA            -> (1172, 1174)\n",
      "##TI            -> (1174, 1176)\n",
      "##ON            -> (1176, 1178)\n",
      "Technical       -> (1180, 1189)\n",
      "Skills          -> (1190, 1196)\n",
      "https           -> (1198, 1203)\n",
      ":               -> (1203, 1204)\n",
      "/               -> (1204, 1205)\n",
      "/               -> (1205, 1206)\n",
      "www             -> (1206, 1209)\n",
      ".               -> (1209, 1210)\n",
      "indeed          -> (1210, 1216)\n",
      ".               -> (1216, 1217)\n",
      "com             -> (1217, 1220)\n",
      "/               -> (1220, 1221)\n",
      "r               -> (1221, 1222)\n",
      "/               -> (1222, 1223)\n",
      "A               -> (1223, 1224)\n",
      "##b             -> (1224, 1225)\n",
      "##his           -> (1225, 1228)\n",
      "##he            -> (1228, 1230)\n",
      "##k             -> (1230, 1231)\n",
      "-               -> (1231, 1232)\n",
      "J               -> (1232, 1233)\n",
      "##ha            -> (1233, 1235)\n",
      "/               -> (1235, 1236)\n",
      "10              -> (1236, 1238)\n",
      "##e             -> (1238, 1239)\n",
      "##7             -> (1239, 1240)\n",
      "##a             -> (1240, 1241)\n",
      "##8             -> (1241, 1242)\n",
      "##c             -> (1242, 1243)\n",
      "##b             -> (1243, 1244)\n",
      "##7             -> (1244, 1245)\n",
      "##32            -> (1245, 1247)\n",
      "##b             -> (1247, 1248)\n",
      "##c             -> (1248, 1249)\n",
      "##43            -> (1249, 1251)\n",
      "##a             -> (1251, 1252)\n",
      "?               -> (1252, 1253)\n",
      "is              -> (1253, 1255)\n",
      "##id            -> (1255, 1257)\n",
      "=               -> (1257, 1258)\n",
      "re              -> (1258, 1260)\n",
      "##x             -> (1260, 1261)\n",
      "-               -> (1261, 1262)\n",
      "download        -> (1262, 1270)\n",
      "&               -> (1270, 1271)\n",
      "i               -> (1271, 1272)\n",
      "##k             -> (1272, 1273)\n",
      "##w             -> (1273, 1274)\n",
      "=               -> (1274, 1275)\n",
      "download        -> (1275, 1283)\n",
      "-               -> (1283, 1284)\n",
      "top             -> (1284, 1287)\n",
      "&               -> (1287, 1288)\n",
      "co              -> (1288, 1290)\n",
      "=               -> (1290, 1291)\n",
      "IN              -> (1291, 1293)\n",
      "â€¢               -> (1296, 1297)\n",
      "Programming     -> (1298, 1309)\n",
      "language        -> (1310, 1318)\n",
      ":               -> (1318, 1319)\n",
      "C               -> (1320, 1321)\n",
      ",               -> (1321, 1322)\n",
      "C               -> (1323, 1324)\n",
      "+               -> (1324, 1325)\n",
      "+               -> (1325, 1326)\n",
      ",               -> (1326, 1327)\n",
      "Java            -> (1328, 1332)\n",
      "â€¢               -> (1333, 1334)\n",
      "Oracle          -> (1335, 1341)\n",
      "People          -> (1342, 1348)\n",
      "##S             -> (1348, 1349)\n",
      "##oft           -> (1349, 1352)\n",
      "â€¢               -> (1353, 1354)\n",
      "Internet        -> (1355, 1363)\n",
      "Of              -> (1364, 1366)\n",
      "Things          -> (1367, 1373)\n",
      "â€¢               -> (1374, 1375)\n",
      "Machine         -> (1376, 1383)\n",
      "Learning        -> (1384, 1392)\n",
      "â€¢               -> (1393, 1394)\n",
      "Database        -> (1395, 1403)\n",
      "Management      -> (1404, 1414)\n",
      "System          -> (1415, 1421)\n",
      "â€¢               -> (1422, 1423)\n",
      "Computer        -> (1424, 1432)\n",
      "Networks        -> (1433, 1441)\n",
      "â€¢               -> (1442, 1443)\n",
      "Operating       -> (1444, 1453)\n",
      "System          -> (1454, 1460)\n",
      "worked          -> (1461, 1467)\n",
      "on              -> (1468, 1470)\n",
      ":               -> (1470, 1471)\n",
      "Linux           -> (1472, 1477)\n",
      ",               -> (1477, 1478)\n",
      "Windows         -> (1479, 1486)\n",
      ",               -> (1486, 1487)\n",
      "Mac             -> (1488, 1491)\n",
      "Non             -> (1493, 1496)\n",
      "-               -> (1497, 1498)\n",
      "Technical       -> (1499, 1508)\n",
      "Skills          -> (1509, 1515)\n",
      "â€¢               -> (1517, 1518)\n",
      "Hon             -> (1519, 1522)\n",
      "##est           -> (1522, 1525)\n",
      "and             -> (1526, 1529)\n",
      "Hard            -> (1530, 1534)\n",
      "-               -> (1534, 1535)\n",
      "Working         -> (1535, 1542)\n",
      "â€¢               -> (1543, 1544)\n",
      "To              -> (1545, 1547)\n",
      "##ler           -> (1547, 1550)\n",
      "##ant           -> (1550, 1553)\n",
      "and             -> (1554, 1557)\n",
      "F               -> (1558, 1559)\n",
      "##lex           -> (1559, 1562)\n",
      "##ible          -> (1562, 1566)\n",
      "to              -> (1567, 1569)\n",
      "Different       -> (1570, 1579)\n",
      "Sit             -> (1580, 1583)\n",
      "##uations       -> (1583, 1590)\n",
      "â€¢               -> (1591, 1592)\n",
      "Pol             -> (1593, 1596)\n",
      "##ite           -> (1596, 1599)\n",
      "and             -> (1600, 1603)\n",
      "Cal             -> (1604, 1607)\n",
      "##m             -> (1607, 1608)\n",
      "â€¢               -> (1609, 1610)\n",
      "Team            -> (1611, 1615)\n",
      "-               -> (1615, 1616)\n",
      "Player          -> (1616, 1622)\n",
      "[SEP]           -> (0, 0)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "resume_text = data[0][\"content\"]\n",
    "annotations = data[0].get(\"annotation\", [])\n",
    "\n",
    "# Flatten all points into (start, end, label) tuples\n",
    "entities = []\n",
    "for ann in annotations:\n",
    "    for point in ann['points']:\n",
    "        entities.append((point['start'], point['end'], ann['label'][0]))\n",
    "\n",
    "# Tokenize the full text with offsets\n",
    "tokens_with_offsets = tokenizer(resume_text, return_offsets_mapping=True, truncation=True)\n",
    "\n",
    "# Show first 10 tokens with offsets\n",
    "for token, offset in zip(tokens_with_offsets.tokens(), tokens_with_offsets[\"offset_mapping\"]):\n",
    "    print(f\"{token:15} -> {offset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ca7c8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A               â†’ B-Name\n",
      "##b             â†’ I-Name\n",
      "##his           â†’ I-Name\n",
      "##he            â†’ I-Name\n",
      "##k             â†’ I-Name\n",
      "J               â†’ I-Name\n",
      "##ha            â†’ I-Name\n",
      "Application     â†’ B-Designation\n",
      "Development     â†’ I-Designation\n",
      "Associate       â†’ I-Designation\n",
      "-               â†’ O\n",
      "A               â†’ B-Companies worked at\n",
      "##cc            â†’ I-Companies worked at\n",
      "##ent           â†’ I-Companies worked at\n",
      "##ure           â†’ I-Companies worked at\n",
      "Bengal          â†’ B-Location\n",
      "##uru           â†’ I-Location\n",
      ",               â†’ O\n",
      "Karnataka       â†’ O\n",
      "-               â†’ O\n",
      "Em              â†’ O\n",
      "##ail           â†’ O\n",
      "me              â†’ O\n",
      "on              â†’ O\n",
      "Indeed          â†’ B-Email Address\n",
      ":               â†’ I-Email Address\n",
      "indeed          â†’ I-Email Address\n",
      ".               â†’ I-Email Address\n",
      "com             â†’ I-Email Address\n",
      "/               â†’ I-Email Address\n"
     ]
    }
   ],
   "source": [
    "aligned_tokens = []\n",
    "aligned_labels = []\n",
    "\n",
    "for token, (tok_start, tok_end) in zip(tokens_with_offsets.tokens(), tokens_with_offsets[\"offset_mapping\"]):\n",
    "    if token in [\"[CLS]\", \"[SEP]\"]:\n",
    "        continue  # Skip special tokens\n",
    "\n",
    "    matched_label = \"O\"\n",
    "\n",
    "    for ent_start, ent_end, ent_label in entities:\n",
    "        if tok_start < ent_end and tok_end > ent_start:\n",
    "            if tok_start == ent_start:\n",
    "                matched_label = \"B-\" + ent_label\n",
    "            else:\n",
    "                matched_label = \"I-\" + ent_label\n",
    "            break\n",
    "\n",
    "    aligned_tokens.append(token)\n",
    "    aligned_labels.append(matched_label)\n",
    "for token, label in zip(aligned_tokens[:30], aligned_labels[:30]):\n",
    "    print(f\"{token:15} â†’ {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a88dbeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-College Name', 'B-Companies worked at', 'B-Designation', 'B-Email Address', 'B-Graduation Year', 'B-Location', 'B-Name', 'B-Skills', 'I-College Name', 'I-Companies worked at', 'I-Designation', 'I-Email Address', 'I-Location', 'I-Name', 'I-Skills', 'O']\n"
     ]
    }
   ],
   "source": [
    "unique_labels = sorted(set(aligned_labels))\n",
    "print(unique_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "003707ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-College Name': 0, 'B-Companies worked at': 1, 'B-Designation': 2, 'B-Email Address': 3, 'B-Graduation Year': 4, 'B-Location': 5, 'B-Name': 6, 'B-Skills': 7, 'I-College Name': 8, 'I-Companies worked at': 9, 'I-Designation': 10, 'I-Email Address': 11, 'I-Location': 12, 'I-Name': 13, 'I-Skills': 14, 'O': 15}\n"
     ]
    }
   ],
   "source": [
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for i, label in enumerate(unique_labels)}\n",
    "print(label2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56bba57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 13, 13, 13, 13, 13, 13, 2, 10, 10, 15, 1, 9, 9, 9, 5, 12, 15, 15, 15, 15, 15, 15, 15, 3, 11, 11, 11, 11, 11]\n"
     ]
    }
   ],
   "source": [
    "label_ids = [label2id[label] for label in aligned_labels]\n",
    "print(label_ids[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5309585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = []\n",
    "all_labels = set()\n",
    "\n",
    "for item in data:\n",
    "    resume_text = item[\"content\"]\n",
    "    annotations = item.get(\"annotation\", [])\n",
    "\n",
    "    entities = []\n",
    "    for ann in annotations:\n",
    "        label = ann.get(\"label\")\n",
    "        if label and len(label) > 0:\n",
    "            for point in ann[\"points\"]:\n",
    "                # Skip noisy long email text (e.g. Indeed links)\n",
    "                if \"@\" in point[\"text\"] and len(point[\"text\"]) > 50:\n",
    "                    continue\n",
    "                entities.append((point[\"start\"], point[\"end\"], label[0]))\n",
    "\n",
    "    tokens_with_offsets = tokenizer(resume_text, return_offsets_mapping=True, truncation=True)\n",
    "\n",
    "    aligned_tokens = []\n",
    "    aligned_labels = []\n",
    "\n",
    "    for token, (tok_start, tok_end) in zip(tokens_with_offsets.tokens(), tokens_with_offsets[\"offset_mapping\"]):\n",
    "        if token in [\"[CLS]\", \"[SEP]\"]:\n",
    "            continue\n",
    "        matched_label = \"O\"\n",
    "        for ent_start, ent_end, ent_label in entities:\n",
    "            if tok_start < ent_end and tok_end > ent_start:\n",
    "                matched_label = \"B-\" + ent_label if tok_start == ent_start else \"I-\" + ent_label\n",
    "                break\n",
    "        aligned_tokens.append(token)\n",
    "        aligned_labels.append(matched_label)\n",
    "        all_labels.add(matched_label)\n",
    "\n",
    "    # âœ… Skip samples with no useful labels\n",
    "    if not any(l != \"O\" for l in aligned_labels):\n",
    "        continue\n",
    "\n",
    "    if aligned_tokens and len(aligned_tokens) == len(aligned_labels):\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(aligned_tokens)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        all_samples.append({\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": aligned_labels\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8f96842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: ['B-College Name', 'B-Companies worked at', 'B-Degree', 'B-Designation', 'B-Email Address', 'B-Graduation Year', 'B-Location', 'B-Name', 'B-Skills', 'B-Years of Experience', 'I-College Name', 'I-Companies worked at', 'I-Degree', 'I-Designation', 'I-Email Address', 'I-Graduation Year', 'I-Location', 'I-Name', 'I-Skills', 'I-Years of Experience', 'O']\n"
     ]
    }
   ],
   "source": [
    "unique_labels = sorted(all_labels)\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "print(\"Unique labels:\", unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "330e9689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input_ids: [138, 1830, 27516, 4638, 1377, 147, 2328, 22491, 3273, 9666, 118, 138, 19515, 3452, 3313, 7756, 12328, 117, 12247, 118, 18653, 11922, 1143, 1113, 10364, 131, 5750, 119, 3254, 120]\n"
     ]
    }
   ],
   "source": [
    "for sample in all_samples:\n",
    "    sample[\"labels\"] = [label2id[label] for label in sample[\"labels\"]]\n",
    "print(\"Sample input_ids:\", all_samples[0][\"input_ids\"][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22e0dca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 176\n",
      "Validation dataset size: 44\n",
      "{'input_ids': [156, 10131, 1179, 24930, 1742, 118, 18653, 11922, 1143, 1113, 10364, 131, 5750, 119, 3254, 120, 187, 120, 156, 10131, 1179, 118, 24930, 1742, 120, 124, 1161, 23249, 1477, 1161, 1559, 1830, 1559, 26752, 1545, 1161, 1559, 22433, 794, 5823, 125, 12577, 1116, 119, 1104, 4600, 1250, 2541, 1107, 13795, 5758, 1111, 6082, 113, 9059, 117, 8410, 114, 1105, 27833, 2394, 794, 13719, 118, 1113, 2541, 1114, 5537, 1216, 1112, 25327, 21906, 2737, 4184, 117, 9190, 5954, 4487, 2772, 117, 1130, 2137, 26643, 117, 1105, 6525, 7921, 18195, 794, 15843, 1107, 3780, 1822, 120, 1344, 118, 6448, 1126, 12512, 2913, 7700, 16548, 1116, 117, 1105, 4795, 5611, 1111, 4683, 1606, 5250, 2430, 2340, 2624, 5537, 794, 5823, 1363, 3044, 1113, 4297, 1329, 2740, 117, 4795, 2801, 117, 111, 20122, 1116, 794, 13719, 118, 1113, 2541, 1114, 145, 19974, 2162, 1571, 111, 24821, 1708, 1495, 117, 1259, 2771, 118, 19089, 25400, 794, 13719, 118, 1113, 2541, 1107, 3780, 155, 24480, 113, 11336, 20080, 4199, 2109, 9059, 118, 4800, 114, 9726, 1116, 794, 5823, 1363, 7401, 1113, 13801, 1105, 3225, 8396, 13770, 794, 2750, 3044, 1113, 6213, 27834, 6806, 2896, 1186, 13284, 794, 13719, 118, 1113, 2541, 1113, 1683, 1654, 6806, 21479, 2249, 794, 18911, 3858, 1200, 117, 2191, 118, 4940, 117, 2463, 118, 9474, 1197, 117, 3023, 13241, 1264, 1591, 1105, 2912, 1106, 1976, 16677, 1106, 1207, 14652, 1105, 7951, 794, 25764, 1637, 1105, 14093, 4909, 4196, 1106, 8306, 7550, 1105, 1103, 1264, 14684, 1193, 794, 5823, 1363, 3645, 1105, 1264, 2635, 4196, 160, 9565, 2428, 142, 3190, 2101, 9637, 17444, 15517, 2036, 25607, 1563, 117, 13801, 118, 1345, 1504, 1106, 13653, 1504, 794, 6955, 1174, 1112, 10331, 8252, 1114, 5096, 2646, 21361, 1116, 3436, 5975, 1121, 13063, 112, 1410, 1106, 20456, 112, 1446, 794, 6955, 1174, 1111, 1305, 1130, 27960, 2961, 1223, 2967, 2653, 26521, 1121, 23844, 112, 1387, 1106, 14125, 112, 1410, 794, 6955, 1174, 1112, 1126, 11300, 9791, 3051, 1223, 1305, 1130, 27960, 1945, 1112, 170, 9059, 25174, 1121, 20456, 112, 1381, 1106, 23844, 112, 1387, 16851, 2339, 9617, 10331, 131, 25327, 21906, 2737, 4184, 117, 25327, 1130, 2137, 26643, 117, 25327, 9190, 5954, 4487, 2772, 117, 25327, 6525, 7921, 18195, 117, 10978, 3794, 2101, 21506, 13801, 2649, 118, 1379, 1504, 1106, 1379, 1504, 10176, 1116, 1105, 24664, 3740, 25583, 794, 7199, 170, 1420, 1105, 12137, 2967, 4795, 2541, 2272, 12146, 4770, 1121, 107, 1109, 11300, 15022, 4800, 2974, 107, 13801, 12983, 16059, 6466, 1233, 18630, 131, 120, 120, 7001, 119, 5750, 119, 3254, 120, 187, 120, 156, 10131, 1179, 118, 24930, 1742, 120, 124, 1161, 23249, 1477, 1161, 1559, 1830, 1559, 26752, 1545, 1161, 1559, 22433, 136, 1110, 2386, 134, 1231, 1775, 118, 9133, 111, 178, 1377, 2246, 134, 9133, 118, 1499, 111, 1884, 134, 15969, 19265, 2896, 1186, 13284, 118, 1333, 1106, 1333, 9059, 14164, 131, 145, 19974, 2162, 113, 126, 114, 117, 24821, 1708, 113, 124, 114, 5967, 5135, 24030, 131, 10616, 25023, 4487, 1643, 2763, 1105, 11336, 27176, 1116, 794, 16625, 12426, 2428, 17516, 1698, 118, 14972, 1104, 138, 8661, 1874, 15253, 118, 14125], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [7, 17, 17, 17, 17, 20, 20, 20, 20, 20, 20, 20, 4, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 20, 20, 9, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 3, 13, 13, 13, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 4, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "train_data, val_data = train_test_split(all_samples, test_size=0.2, random_state=42)\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Validation dataset size:\", len(val_dataset))\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28da031f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Load the model\n",
    "from transformers import BertForTokenClassification\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c52f8eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Softwares\\anaconda\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ner_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fa0f46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Softwares\\anaconda\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  5/132 00:22 < 16:06, 0.13 it/s, Epoch 0.18/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 13\u001b[0m\n\u001b[0;32m      3\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForTokenClassification(tokenizer)\n\u001b[0;32m      4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      5\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      6\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator  \n\u001b[0;32m     10\u001b[0m )\n\u001b[1;32m---> 13\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\Softwares\\anaconda\\Lib\\site-packages\\transformers\\trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2242\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2243\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   2244\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   2245\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2246\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Softwares\\anaconda\\Lib\\site-packages\\transformers\\trainer.py:2599\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2595\u001b[0m         grad_norm \u001b[38;5;241m=\u001b[39m _grad_norm\n\u001b[0;32m   2597\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_pre_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2599\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m   2601\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39moptimizer_step_was_skipped:\n\u001b[0;32m   2604\u001b[0m     \u001b[38;5;66;03m# Delay optimizer scheduling until metrics are generated\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Softwares\\anaconda\\Lib\\site-packages\\accelerate\\optimizer.py:179\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep(closure)\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator_state\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mXLA:\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_state\u001b[38;5;241m.\u001b[39mis_xla_gradients_synced \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Softwares\\anaconda\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:140\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[0;32m    139\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(opt, opt\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Softwares\\anaconda\\Lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    491\u001b[0m             )\n\u001b[1;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Softwares\\anaconda\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Softwares\\anaconda\\Lib\\site-packages\\torch\\optim\\adamw.py:243\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    230\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    232\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    233\u001b[0m         group,\n\u001b[0;32m    234\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    240\u001b[0m         state_steps,\n\u001b[0;32m    241\u001b[0m     )\n\u001b[1;32m--> 243\u001b[0m     adamw(\n\u001b[0;32m    244\u001b[0m         params_with_grad,\n\u001b[0;32m    245\u001b[0m         grads,\n\u001b[0;32m    246\u001b[0m         exp_avgs,\n\u001b[0;32m    247\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    248\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    249\u001b[0m         state_steps,\n\u001b[0;32m    250\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m    251\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    252\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    253\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    254\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    255\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    256\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    257\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    258\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    259\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    260\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    261\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    262\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    263\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    264\u001b[0m     )\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Softwares\\anaconda\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Softwares\\anaconda\\Lib\\site-packages\\torch\\optim\\adamw.py:875\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    873\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 875\u001b[0m func(\n\u001b[0;32m    876\u001b[0m     params,\n\u001b[0;32m    877\u001b[0m     grads,\n\u001b[0;32m    878\u001b[0m     exp_avgs,\n\u001b[0;32m    879\u001b[0m     exp_avg_sqs,\n\u001b[0;32m    880\u001b[0m     max_exp_avg_sqs,\n\u001b[0;32m    881\u001b[0m     state_steps,\n\u001b[0;32m    882\u001b[0m     amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m    883\u001b[0m     beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    884\u001b[0m     beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    885\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m    886\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[0;32m    887\u001b[0m     eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m    888\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[0;32m    889\u001b[0m     capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m    890\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[0;32m    891\u001b[0m     grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[0;32m    892\u001b[0m     found_inf\u001b[38;5;241m=\u001b[39mfound_inf,\n\u001b[0;32m    893\u001b[0m     has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    894\u001b[0m )\n",
      "File \u001b[1;32mc:\\Softwares\\anaconda\\Lib\\site-packages\\torch\\optim\\adamw.py:425\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[0;32m    422\u001b[0m     device_beta1 \u001b[38;5;241m=\u001b[39m beta1\n\u001b[0;32m    424\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m device_beta1)\n\u001b[0;32m    426\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator  \n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8821e172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ner_resume_model\\\\tokenizer_config.json',\n",
       " 'ner_resume_model\\\\special_tokens_map.json',\n",
       " 'ner_resume_model\\\\vocab.txt',\n",
       " 'ner_resume_model\\\\added_tokens.json',\n",
       " 'ner_resume_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Save the processed data\n",
    "model.save_pretrained(\"ner_resume_model\")\n",
    "tokenizer.save_pretrained(\"ner_resume_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc03a8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "# Pick a random resume\n",
    "random_item = random.choice(data)\n",
    "text = random_item[\"content\"]\n",
    "\n",
    "# Tokenize with offset mapping\n",
    "tokens = tokenizer(text, return_offsets_mapping=True, return_tensors=\"pt\", truncation=True)\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "offsets = tokens[\"offset_mapping\"][0]\n",
    "\n",
    "# Inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    predictions = torch.argmax(outputs.logits, dim=2)[0]\n",
    "\n",
    "# Convert predictions to labels\n",
    "predicted_labels = [id2label[p.item()] for p in predictions]\n",
    "\n",
    "# Combine tokens + offsets + labels\n",
    "for token, (start, end), label in zip(tokenizer.convert_ids_to_tokens(input_ids[0]), offsets, predicted_labels):\n",
    "    if label != \"O\":\n",
    "        print(f\"{text[start:end]} â†’ {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4829c9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           |                      | O\n",
      "As              | As                   | O\n",
      "##ish           | ish                  | O\n",
      "Rat             | Rat                  | O\n",
      "##ha            | ha                   | O\n",
      "Sub             | Sub                  | O\n",
      "##ject          | ject                 | O\n",
      "matter          | matter               | O\n",
      "Ex              | Ex                   | O\n",
      "##pert          | pert                 | O\n",
      "-               | -                    | O\n",
      "A               | A                    | O\n",
      "##cc            | cc                   | O\n",
      "##ent           | ent                  | O\n",
      "##ure           | ure                  | O\n",
      "Chennai         | Chennai              | O\n",
      ",               | ,                    | O\n",
      "Tamil           | Tamil                | O\n",
      "Nadu            | Nadu                 | O\n",
      "-               | -                    | O\n",
      "Em              | Em                   | O\n",
      "##ail           | ail                  | O\n",
      "me              | me                   | O\n",
      "on              | on                   | O\n",
      "Indeed          | Indeed               | O\n",
      ":               | :                    | O\n",
      "indeed          | indeed               | O\n",
      ".               | .                    | O\n",
      "com             | com                  | O\n",
      "/               | /                    | O\n",
      "r               | r                    | O\n",
      "/               | /                    | O\n",
      "As              | As                   | O\n",
      "##ish           | ish                  | O\n",
      "-               | -                    | O\n",
      "Rat             | Rat                  | O\n",
      "##ha            | ha                   | O\n",
      "/               | /                    | O\n",
      "85              | 85                   | O\n",
      "##39            | 39                   | O\n",
      "##8             | 8                    | O\n",
      "##8             | 8                    | O\n",
      "##e             | e                    | I-Email Address\n",
      "##0             | 0                    | O\n",
      "##e             | e                    | I-Email Address\n",
      "##0             | 0                    | O\n",
      "##e             | e                    | O\n",
      "##23            | 23                   | O\n",
      "##6             | 6                    | O\n",
      "##a             | a                    | O\n",
      "##3             | 3                    | O\n",
      "W               | W                    | O\n",
      "##OR            | OR                   | O\n",
      "##K             | K                    | O\n",
      "E               | E                    | O\n",
      "##X             | X                    | O\n",
      "##P             | P                    | O\n",
      "##ER            | ER                   | O\n",
      "##IE            | IE                   | O\n",
      "##NC            | NC                   | O\n",
      "##E             | E                    | O\n",
      "Sub             | Sub                  | O\n",
      "##ject          | ject                 | O\n",
      "matter          | matter               | O\n",
      "Ex              | Ex                   | O\n",
      "##pert          | pert                 | O\n",
      "A               | A                    | O\n",
      "##cc            | cc                   | O\n",
      "##ent           | ent                  | O\n",
      "##ure           | ure                  | O\n",
      "-               | -                    | O\n",
      "March           | March                | O\n",
      "2012            | 2012                 | O\n",
      "to              | to                   | O\n",
      "Present         | Present              | O\n",
      "Sub             | Sub                  | O\n",
      "##ject          | ject                 | O\n",
      "matter          | matter               | O\n",
      "expert          | expert               | O\n",
      "E               | E                    | O\n",
      "##D             | D                    | O\n",
      "##UC            | UC                   | O\n",
      "##AT            | AT                   | O\n",
      "##ION           | ION                  | O\n",
      "Be              | Be                   | O\n",
      "##r             | r                    | O\n",
      "##ham           | ham                  | O\n",
      "##pur           | pur                  | O\n",
      "university      | university           | O\n",
      ",               | ,                    | O\n",
      "K               | K                    | O\n",
      "##hall          | hall                 | O\n",
      "##iko           | iko                  | O\n",
      "##te            | te                   | O\n",
      "autonomous      | autonomous           | O\n",
      "college         | college              | O\n",
      "-               | -                    | O\n",
      "B               | B                    | O\n",
      "##rah           | rah                  | O\n",
      "##ma            | ma                   | O\n",
      "##pur           | pur                  | O\n",
      ",               | ,                    | O\n",
      "Or              | Or                   | O\n",
      "##issa          | issa                 | O\n",
      "SK              | SK                   | O\n",
      "##IL            | IL                   | O\n",
      "##LS            | LS                   | O\n",
      "In              | In                   | O\n",
      "##vo            | vo                   | O\n",
      "##ice           | ice                  | O\n",
      "(               | (                    | O\n",
      "5               | 5                    | O\n",
      "years           | years                | O\n",
      ")               | )                    | O\n",
      ",               | ,                    | O\n",
      "posting         | posting              | O\n",
      ".               | .                    | O\n",
      "(               | (                    | O\n",
      "5               | 5                    | O\n",
      "years           | years                | O\n",
      ")               | )                    | O\n",
      ",               | ,                    | O\n",
      "T               | T                    | O\n",
      "##RA            | RA                   | O\n",
      "##IN            | IN                   | O\n",
      "##ING           | ING                  | O\n",
      "(               | (                    | O\n",
      "4               | 4                    | O\n",
      "years           | years                | O\n",
      ")               | )                    | O\n",
      "AD              | AD                   | O\n",
      "##DI            | DI                   | O\n",
      "##TI            | TI                   | O\n",
      "##ON            | ON                   | O\n",
      "##AL            | AL                   | O\n",
      "IN              | IN                   | O\n",
      "##F             | F                    | O\n",
      "##OR            | OR                   | O\n",
      "##MA            | MA                   | O\n",
      "##TI            | TI                   | O\n",
      "##ON            | ON                   | O\n",
      "SK              | SK                   | O\n",
      "##IL            | IL                   | O\n",
      "##LS            | LS                   | O\n",
      "In              | In                   | O\n",
      "##vo            | vo                   | O\n",
      "##ice           | ice                  | O\n",
      "processing      | processing           | O\n",
      ",               | ,                    | O\n",
      "Team            | Team                 | O\n",
      "handling        | handling             | O\n",
      ",               | ,                    | O\n",
      "new             | new                  | O\n",
      "join            | join                 | O\n",
      "##ers           | ers                  | O\n",
      "training        | training             | O\n",
      ".               | .                    | O\n",
      "sa              | sa                   | O\n",
      "##p             | p                    | O\n",
      "posting         | posting              | O\n",
      ",               | ,                    | O\n",
      "vendor          | vendor               | O\n",
      "call            | call                 | O\n",
      "attend          | attend               | O\n",
      "and             | and                  | O\n",
      "resolve         | resolve              | O\n",
      "the             | the                  | O\n",
      "issue           | issue                | O\n",
      ",               | ,                    | O\n",
      "meet            | meet                 | O\n",
      "SL              | SL                   | O\n",
      "##A             | A                    | O\n",
      "ta              | ta                   | O\n",
      "##t             | t                    | O\n",
      ",               | ,                    | O\n",
      "working         | working              | O\n",
      "with            | with                 | O\n",
      "client          | client               | O\n",
      "tool            | tool                 | O\n",
      ".               | .                    | O\n",
      "https           | https                | O\n",
      ":               | :                    | O\n",
      "/               | /                    | O\n",
      "/               | /                    | O\n",
      "www             | www                  | O\n",
      ".               | .                    | O\n",
      "indeed          | indeed               | O\n",
      ".               | .                    | O\n",
      "com             | com                  | O\n",
      "/               | /                    | O\n",
      "r               | r                    | O\n",
      "/               | /                    | O\n",
      "As              | As                   | O\n",
      "##ish           | ish                  | O\n",
      "-               | -                    | O\n",
      "Rat             | Rat                  | O\n",
      "##ha            | ha                   | O\n",
      "/               | /                    | O\n",
      "85              | 85                   | O\n",
      "##39            | 39                   | O\n",
      "##8             | 8                    | O\n",
      "##8             | 8                    | O\n",
      "##e             | e                    | O\n",
      "##0             | 0                    | O\n",
      "##e             | e                    | O\n",
      "##0             | 0                    | O\n",
      "##e             | e                    | O\n",
      "##23            | 23                   | O\n",
      "##6             | 6                    | O\n",
      "##a             | a                    | O\n",
      "##3             | 3                    | O\n",
      "?               | ?                    | O\n",
      "is              | is                   | O\n",
      "##id            | id                   | O\n",
      "=               | =                    | O\n",
      "re              | re                   | O\n",
      "##x             | x                    | O\n",
      "-               | -                    | O\n",
      "download        | download             | O\n",
      "&               | &                    | O\n",
      "i               | i                    | O\n",
      "##k             | k                    | O\n",
      "##w             | w                    | O\n",
      "=               | =                    | O\n",
      "download        | download             | O\n",
      "-               | -                    | O\n",
      "top             | top                  | O\n",
      "&               | &                    | O\n",
      "co              | co                   | O\n",
      "=               | =                    | O\n",
      "IN              | IN                   | O\n",
      "[SEP]           |                      | O\n"
     ]
    }
   ],
   "source": [
    "custom_text = custom_text = data[10][\"content\"]\n",
    "\n",
    "\n",
    "\n",
    "tokens = tokenizer(custom_text, return_offsets_mapping=True, return_tensors=\"pt\", truncation=True)\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "offsets = tokens[\"offset_mapping\"][0]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    predictions = torch.argmax(outputs.logits, dim=2)[0]\n",
    "\n",
    "predicted_labels = [id2label[p.item()] for p in predictions]\n",
    "\n",
    "for token, (start, end), label in zip(tokenizer.convert_ids_to_tokens(input_ids[0]), offsets, predicted_labels):\n",
    "    word_piece = custom_text[start:end]\n",
    "    print(f\"{token:15} | {word_piece:20} | {label}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48b0ea81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O', 'I-Email Address'}\n"
     ]
    }
   ],
   "source": [
    "# Group token-level BIO predictions into full entities\n",
    "entities = []\n",
    "current_entity = \"\"\n",
    "current_label = None\n",
    "\n",
    "for (start, end), label in zip(offsets, predicted_labels):\n",
    "    word = custom_text[start:end]\n",
    "    if label.startswith(\"B-\"):\n",
    "        # Save previous if exists\n",
    "        if current_entity:\n",
    "            entities.append((current_entity.strip(), current_label))\n",
    "        current_entity = word\n",
    "        current_label = label[2:]  # Remove B-\n",
    "    elif label.startswith(\"I-\") and current_label == label[2:]:\n",
    "        current_entity += \" \" + word\n",
    "    else:\n",
    "        if current_entity:\n",
    "            entities.append((current_entity.strip(), current_label))\n",
    "            current_entity = \"\"\n",
    "            current_label = None\n",
    "\n",
    "# Add last entity if needed\n",
    "if current_entity:\n",
    "    entities.append((current_entity.strip(), current_label))\n",
    "\n",
    "# Print results\n",
    "for ent, label in entities:\n",
    "    print(f\"{ent:30} â†’ {label}\")\n",
    "\n",
    "print(set(predicted_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a3ee400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'O': 80353, 'I-Email Address': 5688, 'I-Skills': 4858, 'I-Designation': 1388, 'I-Companies worked at': 1025, 'I-College Name': 881, 'I-Name': 864, 'I-Degree': 597, 'I-Location': 428, 'B-Companies worked at': 414, 'B-Designation': 409, 'B-Location': 353, 'B-Name': 221, 'B-Email Address': 202, 'B-Skills': 159, 'B-College Name': 157, 'B-Degree': 141, 'B-Graduation Year': 119, 'I-Years of Experience': 82, 'B-Years of Experience': 27, 'I-Graduation Year': 16})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "flat_labels = []\n",
    "for sample in all_samples:\n",
    "    flat_labels.extend(sample[\"labels\"])\n",
    "\n",
    "flat_labels_named = [id2label[i] for i in flat_labels]\n",
    "print(Counter(flat_labels_named))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba908b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\softwares\\anaconda\\lib\\site-packages (from seqeval) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\softwares\\anaconda\\lib\\site-packages (from seqeval) (1.5.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\softwares\\anaconda\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\softwares\\anaconda\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\softwares\\anaconda\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16247 sha256=9b9b336b851b6b0ea99c5d54eff02381a845d5b90e5d304f9c6c4e7d57723746\n",
      "  Stored in directory: c:\\users\\rouna\\appdata\\local\\pip\\cache\\wheels\\5f\\b8\\73\\0b2c1a76b701a677653dd79ece07cfabd7457989dbfbdcd8d7\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e3b327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "\n",
    "    true_labels = []\n",
    "    true_predictions = []\n",
    "\n",
    "    for pred, label in zip(predictions, labels):\n",
    "        temp_labels = []\n",
    "        temp_preds = []\n",
    "        for p_i, l_i in zip(pred, label):\n",
    "            if l_i != -100:  # ignore padded values\n",
    "                temp_labels.append(id2label[l_i])\n",
    "                temp_preds.append(id2label[p_i])\n",
    "        true_labels.append(temp_labels)\n",
    "        true_predictions.append(temp_preds)\n",
    "\n",
    "    print(classification_report(true_labels, true_predictions))\n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a8775d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Softwares\\anaconda\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "       College Name       0.09      0.29      0.14        24\n",
      "Companies worked at       0.35      0.33      0.34        82\n",
      "             Degree       0.37      0.26      0.30        27\n",
      "        Designation       0.48      0.44      0.46        97\n",
      "      Email Address       0.25      0.31      0.28        51\n",
      "    Graduation Year       0.00      0.00      0.00        25\n",
      "           Location       0.57      0.48      0.53        64\n",
      "               Name       0.64      0.63      0.64        46\n",
      "             Skills       0.04      0.09      0.05        35\n",
      "Years of Experience       0.20      0.20      0.20         5\n",
      "\n",
      "          micro avg       0.33      0.36      0.34       456\n",
      "          macro avg       0.30      0.30      0.29       456\n",
      "       weighted avg       0.37      0.36      0.36       456\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.36635497212409973,\n",
       " 'eval_model_preparation_time': 0.0009,\n",
       " 'eval_precision': 0.32669322709163345,\n",
       " 'eval_recall': 0.35964912280701755,\n",
       " 'eval_f1': 0.3423799582463466,\n",
       " 'eval_runtime': 11.8277,\n",
       " 'eval_samples_per_second': 3.72,\n",
       " 'eval_steps_per_second': 0.507}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics  # âœ… New line here\n",
    ")\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4137a23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-Name', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "custom_text = \"\"\"\n",
    "Rounak Laddha  \n",
    "Machine Learning Engineer - Walmart  \n",
    "Chicago, IL  \n",
    "Email: rounak@example.com  \n",
    "Skills: Python, SQL, Pandas, NumPy\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "tokens = tokenizer(custom_text, return_offsets_mapping=True, return_tensors=\"pt\", truncation=True)\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "offsets = tokens[\"offset_mapping\"][0]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    predictions = torch.argmax(outputs.logits, dim=2)[0]\n",
    "\n",
    "predicted_labels = [id2label[p.item()] for p in predictions]\n",
    "\n",
    "# Group BIO labels into full spans\n",
    "entities = []\n",
    "current_entity = \"\"\n",
    "current_label = None\n",
    "\n",
    "for (start, end), label in zip(offsets, predicted_labels):\n",
    "    word = custom_text[start:end]\n",
    "    if label.startswith(\"B-\"):\n",
    "        if current_entity:\n",
    "            entities.append((current_entity.strip(), current_label))\n",
    "        current_entity = word\n",
    "        current_label = label[2:]\n",
    "    elif label.startswith(\"I-\") and current_label == label[2:]:\n",
    "        current_entity += \" \" + word\n",
    "    else:\n",
    "        if current_entity:\n",
    "            entities.append((current_entity.strip(), current_label))\n",
    "            current_entity = \"\"\n",
    "            current_label = None\n",
    "\n",
    "if current_entity:\n",
    "    entities.append((current_entity.strip(), current_label))\n",
    "\n",
    "print(predicted_labels[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3365c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§ª Token-Level Prediction Preview:\n",
      "[CLS]           |                      | B-Name\n",
      "Man             | Man                  | O\n",
      "##ish           | ish                  | O\n",
      "##a             | a                    | O\n",
      "B               | B                    | I-Name\n",
      "##hart          | hart                 | I-Name\n",
      "##i             | i                    | O\n",
      "Software        | Software             | O\n",
      "Auto            | Auto                 | I-Designation\n",
      "##mation        | mation               | I-Designation\n",
      "Engineer        | Engineer             | I-Designation\n",
      "Pune            | Pune                 | B-Location\n",
      ",               | ,                    | O\n",
      "Maharashtra     | Maharashtra          | O\n",
      "-               | -                    | O\n",
      "Em              | Em                   | O\n",
      "##ail           | ail                  | O\n",
      "me              | me                   | O\n",
      "on              | on                   | O\n",
      "Indeed          | Indeed               | O\n",
      ":               | :                    | O\n",
      "indeed          | indeed               | B-Email Address\n",
      ".               | .                    | I-Email Address\n",
      "com             | com                  | I-Email Address\n",
      "/               | /                    | I-Email Address\n",
      "r               | r                    | I-Email Address\n",
      "/               | /                    | I-Email Address\n",
      "Man             | Man                  | I-Email Address\n",
      "##ish           | ish                  | I-Email Address\n",
      "##a             | a                    | I-Email Address\n",
      "-               | -                    | I-Email Address\n",
      "B               | B                    | I-Email Address\n",
      "##hart          | hart                 | O\n",
      "##i             | i                    | I-Email Address\n",
      "/               | /                    | I-Email Address\n",
      "35              | 35                   | I-Email Address\n",
      "##7             | 7                    | I-Email Address\n",
      "##3             | 3                    | I-Email Address\n",
      "##e             | e                    | I-Email Address\n",
      "##36            | 36                   | I-Email Address\n",
      "##0             | 0                    | I-Email Address\n",
      "##8             | 8                    | I-Email Address\n",
      "##8             | 8                    | I-Email Address\n",
      "##dd            | dd                   | I-Email Address\n",
      "##c             | c                    | I-Email Address\n",
      "##0             | 0                    | I-Email Address\n",
      "##7             | 7                    | I-Email Address\n",
      "##3             | 3                    | I-Email Address\n",
      "â€¢               | â€¢                    | O\n",
      "3               | 3                    | O\n",
      ".               | .                    | I-Years of Experience\n",
      "5               | 5                    | I-Years of Experience\n",
      "years           | years                | I-Years of Experience\n",
      "of              | of                   | O\n",
      "professional    | professional         | O\n",
      "IT              | IT                   | O\n",
      "experience      | experience           | O\n",
      "in              | in                   | O\n",
      "Banking         | Banking              | O\n",
      "and             | and                  | O\n",
      "Finance         | Finance              | O\n",
      "domain          | domain               | O\n",
      "and             | and                  | O\n",
      "currently       | currently            | O\n",
      "working         | working              | O\n",
      "as              | as                   | O\n",
      "Software        | Software             | O\n",
      "Auto            | Auto                 | O\n",
      "##mation        | mation               | O\n",
      "Engineer        | Engineer             | O\n",
      "in              | in                   | O\n",
      "In              | In                   | O\n",
      "##fo            | fo                   | O\n",
      "##sy            | sy                   | O\n",
      "##s             | s                    | O\n",
      "Limited         | Limited              | O\n",
      ",               | ,                    | O\n",
      "Pune            | Pune                 | O\n",
      ".               | .                    | O\n",
      "â€¢               | â€¢                    | O\n",
      "Have            | Have                 | O\n",
      "experience      | experience           | O\n",
      "in              | in                   | O\n",
      "accounts        | accounts             | O\n",
      "and             | and                  | O\n",
      "customers       | customers            | O\n",
      "domain          | domain               | O\n",
      "in              | in                   | O\n",
      "banking         | banking              | O\n",
      ".               | .                    | O\n",
      "â€¢               | â€¢                    | O\n",
      "W               | W                    | O\n",
      "##oki           | oki                  | O\n",
      "##ng            | ng                   | O\n",
      "on              | on                   | O\n",
      "S               | S                    | O\n",
      "##OA            | OA                   | O\n",
      "technology      | technology           | O\n",
      ".               | .                    | O\n",
      "â€¢               | â€¢                    | O\n",
      "Hands           | Hands                | O\n",
      "on              | on                   | O\n",
      "experience      | experience           | O\n",
      "of              | of                   | O\n",
      "2               | 2                    | O\n",
      "+               | +                    | O\n",
      "years           | years                | O\n",
      "in              | in                   | O\n",
      "Oracle          | Oracle               | O\n",
      "11              | 11                   | O\n",
      "##g             | g                    | O\n",
      "â€¢               | â€¢                    | O\n",
      "2               | 2                    | O\n",
      ".               | .                    | O\n",
      "9               | 9                    | O\n",
      "years           | years                | O\n",
      "of              | of                   | O\n",
      "professional    | professional         | O\n",
      "experience      | experience           | O\n",
      "in              | in                   | O\n",
      "Middle          | Middle               | O\n",
      "##ware          | ware                 | O\n",
      "Testing         | Testing              | O\n",
      "and             | and                  | O\n",
      "Fun             | Fun                  | O\n",
      "##ctional       | ctional              | O\n",
      "Testing         | Testing              | O\n",
      "â€¢               | â€¢                    | O\n",
      "4               | 4                    | O\n",
      "months          | months               | O\n",
      "of              | of                   | O\n",
      "experience      | experience           | O\n",
      "with            | with                 | O\n",
      "U               | U                    | O\n",
      "##i             | i                    | O\n",
      "##P             | P                    | O\n",
      "##ath           | ath                  | O\n",
      ".               | .                    | O\n",
      "â€¢               | â€¢                    | O\n",
      "Experience      | Experience           | O\n",
      "on              | on                   | O\n",
      "G               | G                    | O\n",
      "##UI            | UI                   | O\n",
      "and             | and                  | O\n",
      "API             | API                  | O\n",
      "testing         | testing              | O\n",
      "on              | on                   | O\n",
      "HP              | HP                   | O\n",
      "U               | U                    | O\n",
      "##FT            | FT                   | O\n",
      "â€¢               | â€¢                    | O\n",
      "Working         | Working              | O\n",
      "on              | on                   | O\n",
      "a               | a                    | O\n",
      "##gi            | gi                   | O\n",
      "##le            | le                   | O\n",
      "methodology     | methodology          | O\n",
      "where           | where                | O\n",
      "involved        | involved             | O\n",
      "as              | as                   | O\n",
      "a               | a                    | O\n",
      "senior          | senior               | O\n",
      "test            | test                 | O\n",
      "##er            | er                   | O\n",
      ".               | .                    | O\n",
      "â€¢               | â€¢                    | O\n",
      "In              | In                   | O\n",
      "##volved        | volved               | O\n",
      "in              | in                   | O\n",
      "various         | various              | O\n",
      "ST              | ST                   | O\n",
      "##LC            | LC                   | O\n",
      "stages          | stages               | O\n",
      "including       | including            | O\n",
      "Test            | Test                 | O\n",
      "Planning        | Planning             | O\n",
      ",               | ,                    | O\n",
      "Test            | Test                 | O\n",
      "analysis        | analysis             | O\n",
      ",               | ,                    | O\n",
      "Test            | Test                 | O\n",
      "Ex              | Ex                   | O\n",
      "##ec            | ec                   | O\n",
      "##ution         | ution                | O\n",
      ",               | ,                    | O\n",
      "De              | De                   | O\n",
      "##fect          | fect                 | O\n",
      "management      | management           | O\n",
      "and             | and                  | O\n",
      "Test            | Test                 | O\n",
      "reporting       | reporting            | O\n",
      ".               | .                    | O\n",
      "â€¢               | â€¢                    | O\n",
      "Po              | Po                   | O\n",
      "##sses          | sses                 | O\n",
      "##s             | s                    | O\n",
      "sound           | sound                | O\n",
      "knowledge       | knowledge            | O\n",
      "of              | of                   | O\n",
      "S               | S                    | O\n",
      "##QL            | QL                   | O\n",
      ",               | ,                    | O\n",
      "ST              | ST                   | O\n",
      "##LC            | LC                   | O\n",
      ",               | ,                    | O\n",
      "Testing         | Testing              | O\n",
      "Pro             | Pro                  | O\n",
      "##ced           | ced                  | O\n",
      "##ures          | ures                 | O\n",
      ",               | ,                    | O\n",
      "HP              | HP                   | O\n",
      "AL              | AL                   | O\n",
      "##M             | M                    | O\n",
      ",               | ,                    | O\n",
      "HP              | HP                   | O\n",
      "U               | U                    | O\n",
      "##FT            | FT                   | O\n",
      ",               | ,                    | O\n",
      "HP              | HP                   | O\n",
      "SV              | SV                   | O\n",
      ",               | ,                    | O\n",
      "S               | S                    | O\n",
      "##OA            | OA                   | O\n",
      "##P             | P                    | O\n",
      "U               | U                    | O\n",
      "##I             | I                    | O\n",
      ",               | ,                    | O\n",
      "J               | J                    | O\n",
      "##IR            | IR                   | O\n",
      "##A             | A                    | O\n",
      ",               | ,                    | O\n",
      "J               | J                    | O\n",
      "##EN            | EN                   | O\n",
      "##K             | K                    | O\n",
      "##IN            | IN                   | O\n",
      "##S             | S                    | O\n",
      ",               | ,                    | O\n",
      "C               | C                    | O\n",
      "##IC            | IC                   | O\n",
      "##D             | D                    | O\n",
      ",               | ,                    | O\n",
      "U               | U                    | O\n",
      "##i             | i                    | O\n",
      "##P             | P                    | O\n",
      "##ath           | ath                  | O\n",
      ".               | .                    | O\n",
      "â€¢               | â€¢                    | O\n",
      "In              | In                   | O\n",
      "##volved        | volved               | O\n",
      "in              | in                   | O\n",
      "various         | various              | O\n",
      "client          | client               | O\n",
      "presentation    | presentation         | O\n",
      ".               | .                    | O\n",
      "Training        | Training             | O\n",
      "&               | &                    | O\n",
      "Achievement     | Achievement          | O\n",
      "Title           | Title                | O\n",
      ":               | :                    | O\n",
      "In              | In                   | O\n",
      "##fo            | fo                   | O\n",
      "##sy            | sy                   | O\n",
      "##s             | s                    | O\n",
      "E               | E                    | O\n",
      "&               | &                    | O\n",
      "R               | R                    | O\n",
      "Training        | Training             | O\n",
      "Des             | Des                  | O\n",
      "##cription      | cription             | O\n",
      ":               | :                    | O\n",
      "Has             | Has                  | O\n",
      "undergone       | undergone            | O\n",
      "E               | E                    | O\n",
      "&               | &                    | O\n",
      "R               | R                    | O\n",
      "training        | training             | O\n",
      "in              | in                   | O\n",
      "In              | In                   | O\n",
      "##fo            | fo                   | O\n",
      "##sy            | sy                   | O\n",
      "##s             | s                    | O\n",
      "Limited         | Limited              | O\n",
      "(               | (                    | O\n",
      "Mysore          | Mysore               | O\n",
      ")               | )                    | O\n",
      "in              | in                   | O\n",
      "Microsoft       | Microsoft            | O\n",
      ".               | .                    | O\n",
      "Net             | Net                  | O\n",
      "Stream          | Stream               | O\n",
      ".               | .                    | O\n",
      "There           | There                | O\n",
      "I               | I                    | O\n",
      "had             | had                  | O\n",
      "been            | been                 | O\n",
      "explored        | explored             | O\n",
      "S               | S                    | O\n",
      "##QL            | QL                   | O\n",
      ",               | ,                    | O\n",
      "R               | R                    | O\n",
      "##D             | D                    | O\n",
      "##BM            | BM                   | O\n",
      "##S             | S                    | O\n",
      ",               | ,                    | O\n",
      "O               | O                    | O\n",
      "##OP            | OP                   | O\n",
      "##S             | S                    | O\n",
      ",               | ,                    | O\n",
      "Main            | Main                 | O\n",
      "##frame         | frame                | O\n",
      "##s             | s                    | O\n",
      ",               | ,                    | O\n",
      "Software        | Software             | O\n",
      "Testing         | Testing              | O\n",
      "and             | and                  | O\n",
      "Software        | Software             | O\n",
      "Engineering     | Engineering          | O\n",
      ".               | .                    | O\n",
      "Has             | Has                  | O\n",
      "been            | been                 | O\n",
      "trained         | trained              | O\n",
      "in              | in                   | O\n",
      "Auto            | Auto                 | O\n",
      "##mation        | mation               | O\n",
      "Testing         | Testing              | O\n",
      "Too             | Too                  | O\n",
      "##ls            | ls                   | O\n",
      "used            | used                 | O\n",
      "-               | -                    | O\n",
      "Eclipse         | Eclipse              | O\n",
      ",               | ,                    | O\n",
      "U               | U                    | O\n",
      "##FT            | FT                   | O\n",
      ",               | ,                    | O\n",
      "R               | R                    | O\n",
      "##P             | P                    | O\n",
      "##T             | T                    | O\n",
      ",               | ,                    | O\n",
      "S               | S                    | O\n",
      "##QL            | QL                   | O\n",
      "Server          | Server               | O\n",
      "Studio          | Studio               | O\n",
      "Re              | Re                   | O\n",
      "##ceived        | ceived               | O\n",
      "two             | two                  | O\n",
      "times           | times                | O\n",
      "F               | F                    | O\n",
      "##S             | S                    | O\n",
      "IN              | IN                   | O\n",
      "##ST            | ST                   | O\n",
      "##A             | A                    | O\n",
      "award           | award                | O\n",
      "from            | from                 | O\n",
      "In              | In                   | O\n",
      "##fo            | fo                   | O\n",
      "##sy            | sy                   | O\n",
      "##s             | s                    | O\n",
      "for             | for                  | O\n",
      "excellence      | excellence           | O\n",
      "in              | in                   | O\n",
      "work            | work                 | O\n",
      "in              | in                   | O\n",
      "automation      | automation           | O\n",
      "and             | and                  | O\n",
      "team            | team                 | O\n",
      "support         | support              | O\n",
      "Got             | Got                  | O\n",
      "A               | A                    | O\n",
      "##pp            | pp                   | O\n",
      "##re            | re                   | O\n",
      "##ciation       | ciation              | O\n",
      "from            | from                 | O\n",
      "Project         | Project              | O\n",
      "Manager         | Manager              | O\n",
      "for             | for                  | O\n",
      "root            | root                 | O\n",
      "cause           | cause                | O\n",
      "analysis        | analysis             | O\n",
      "of              | of                   | O\n",
      "defects         | defects              | O\n",
      "Got             | Got                  | O\n",
      "C               | C                    | O\n",
      "##lient         | lient                | O\n",
      "A               | A                    | O\n",
      "##pp            | pp                   | O\n",
      "##re            | re                   | O\n",
      "##ciation       | ciation              | O\n",
      "##s             | s                    | O\n",
      "for             | for                  | O\n",
      "successful      | successful           | O\n",
      "execution       | execution            | O\n",
      "in              | in                   | O\n",
      "releases        | releases             | O\n",
      ".               | .                    | O\n",
      "(               | (                    | O\n",
      "Almost          | Almost               | O\n",
      "240             | 240                  | O\n",
      "service         | service              | O\n",
      "operations      | operations           | O\n",
      "go              | go                   | O\n",
      "live            | live                 | O\n",
      "in              | in                   | O\n",
      "a               | a                    | O\n",
      "year            | year                 | O\n",
      ".               | .                    | O\n",
      ")               | )                    | O\n",
      "Will            | Will                 | O\n",
      "##ing           | ing                  | O\n",
      "to              | to                   | O\n",
      "relocate        | relocate             | O\n",
      "to              | to                   | O\n",
      ":               | :                    | O\n",
      "Pune            | Pune                 | O\n",
      ",               | ,                    | O\n",
      "Maharashtra     | Maharashtra          | O\n",
      "W               | W                    | O\n",
      "##OR            | OR                   | O\n",
      "##K             | K                    | O\n",
      "E               | E                    | O\n",
      "##X             | X                    | O\n",
      "##P             | P                    | O\n",
      "##ER            | ER                   | O\n",
      "##IE            | IE                   | O\n",
      "##NC            | NC                   | O\n",
      "##E             | E                    | O\n",
      "NO              | NO                   | O\n",
      "##T             | T                    | O\n",
      "W               | W                    | O\n",
      "##OR            | OR                   | O\n",
      "##K             | K                    | O\n",
      "##ING           | ING                  | O\n",
      "Software        | Software             | O\n",
      "Auto            | Auto                 | O\n",
      "##mation        | mation               | O\n",
      "Engineer        | Engineer             | O\n",
      "In              | In                   | O\n",
      "##fo            | fo                   | O\n",
      "##sy            | sy                   | O\n",
      "##s             | s                    | O\n",
      "Limited         | Limited              | O\n",
      "-               | -                    | O\n",
      "August          | August               | O\n",
      "2014            | 2014                 | O\n",
      "to              | to                   | O\n",
      "July            | July                 | O\n",
      "2017            | 2017                 | O\n",
      "-               | -                    | O\n",
      ">               | >                    | O\n",
      "Work            | Work                 | O\n",
      "##ed            | ed                   | O\n",
      "as              | as                   | O\n",
      "an              | an                   | O\n",
      "software        | software             | O\n",
      "automation      | automation           | O\n",
      "test            | test                 | O\n",
      "##er            | er                   | O\n",
      "more            | more                 | O\n",
      "than            | than                 | O\n",
      "3               | 3                    | O\n",
      "years           | years                | O\n",
      ".               | .                    | O\n",
      "-               | -                    | O\n",
      ">               | >                    | O\n",
      "Working         | Working              | O\n",
      "experience      | experience           | O\n",
      "in              | in                   | O\n",
      "A               | A                    | O\n",
      "##gi            | gi                   | O\n",
      "##le            | le                   | O\n",
      "methodology     | methodology          | O\n",
      ".               | .                    | O\n",
      "https           | https                | O\n",
      ":               | :                    | O\n",
      "/               | /                    | O\n",
      "/               | /                    | O\n",
      "www             | www                  | O\n",
      ".               | .                    | O\n",
      "indeed          | indeed               | O\n",
      ".               | .                    | O\n",
      "com             | com                  | O\n",
      "/               | /                    | O\n",
      "r               | r                    | O\n",
      "/               | /                    | O\n",
      "Man             | Man                  | O\n",
      "##ish           | ish                  | O\n",
      "##a             | a                    | O\n",
      "-               | -                    | O\n",
      "B               | B                    | O\n",
      "##hart          | hart                 | O\n",
      "##i             | i                    | O\n",
      "/               | /                    | O\n",
      "35              | 35                   | O\n",
      "##7             | 7                    | O\n",
      "##3             | 3                    | O\n",
      "##e             | e                    | O\n",
      "##36            | 36                   | O\n",
      "##0             | 0                    | O\n",
      "##8             | 8                    | O\n",
      "##8             | 8                    | O\n",
      "##dd            | dd                   | O\n",
      "##c             | c                    | O\n",
      "##0             | 0                    | O\n",
      "##7             | 7                    | O\n",
      "##3             | 3                    | O\n",
      "?               | ?                    | O\n",
      "is              | is                   | O\n",
      "##id            | id                   | O\n",
      "=               | =                    | O\n",
      "re              | re                   | O\n",
      "##x             | x                    | O\n",
      "-               | -                    | O\n",
      "[SEP]           |                      | O\n",
      "\n",
      "ðŸ“„ Extracted Entities:\n",
      "Pune                           â†’ Location\n",
      "indeed . com / r / Man ish a - B â†’ Email Address\n"
     ]
    }
   ],
   "source": [
    "custom_text = data[42][\"content\"]  # or use any resume-style text\n",
    "\n",
    "# Step 1: Tokenize with offset mapping\n",
    "tokens = tokenizer(custom_text, return_offsets_mapping=True, return_tensors=\"pt\", truncation=True)\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "offsets = tokens[\"offset_mapping\"][0]\n",
    "\n",
    "# Step 2: Predict token-level labels\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    predictions = torch.argmax(outputs.logits, dim=2)[0]\n",
    "\n",
    "predicted_labels = [id2label[p.item()] for p in predictions]\n",
    "\n",
    "# âœ… Step 3: Diagnostic print to check whatâ€™s going on\n",
    "print(\"\\nðŸ§ª Token-Level Prediction Preview:\")\n",
    "for token, (start, end), label in zip(tokenizer.convert_ids_to_tokens(input_ids[0]), offsets, predicted_labels):\n",
    "    word = custom_text[start:end].strip()\n",
    "    print(f\"{token:15} | {word:20} | {label}\")\n",
    "\n",
    "# âœ… Step 4: Group BIO labels into full entity spans\n",
    "entities = []\n",
    "current_entity = \"\"\n",
    "current_label = None\n",
    "\n",
    "for (start, end), label in zip(offsets, predicted_labels):\n",
    "    word = custom_text[start:end].strip()\n",
    "    if not word:  # if offset gives blank, fallback to token itself\n",
    "        continue\n",
    "\n",
    "    if label.startswith(\"B-\"):\n",
    "        if current_entity:\n",
    "            entities.append((current_entity.strip(), current_label))\n",
    "        current_entity = word\n",
    "        current_label = label[2:]\n",
    "\n",
    "    elif label.startswith(\"I-\") and current_label == label[2:]:\n",
    "        current_entity += \" \" + word\n",
    "\n",
    "    else:\n",
    "        if current_entity:\n",
    "            entities.append((current_entity.strip(), current_label))\n",
    "            current_entity = \"\"\n",
    "            current_label = None\n",
    "\n",
    "# Add final entity if one was being built\n",
    "if current_entity:\n",
    "    entities.append((current_entity.strip(), current_label))\n",
    "\n",
    "# âœ… Step 5: Print final extracted entities\n",
    "print(\"\\nðŸ“„ Extracted Entities:\")\n",
    "if entities:\n",
    "    for ent, label in entities:\n",
    "        print(f\"{ent:30} â†’ {label}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No entities extracted. Try another input or check predictions above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7966d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Companies worked at', 463), ('Skills', 360), ('Designation', 278), ('Location', 223), ('College Name', 189), ('Graduation Year', 181), ('Degree', 156), ('Email Address', 113), ('Name', 101), ('Years of Experience', 22), ('UNKNOWN', 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "label_counter = Counter()\n",
    "with open(\"Entity Recognition in Resumes.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 100: break\n",
    "        item = json.loads(line)\n",
    "        for ann in item.get(\"annotation\", []):\n",
    "            label_counter.update(ann.get(\"label\", []))\n",
    "\n",
    "print(label_counter.most_common())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "098b84e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_tokens_with_labels(text, annotations, tokenizer):\n",
    "    # Extract entity spans from annotations\n",
    "    entities = []\n",
    "    for ann in annotations:\n",
    "        for point in ann['points']:\n",
    "            start = point['start']\n",
    "            end = point['end']\n",
    "            label = ann['label'][0]\n",
    "            entities.append((start, end, label))\n",
    "    \n",
    "    # Tokenize text with offsets\n",
    "    tokens_data = tokenizer(text, return_offsets_mapping=True, truncation=True)\n",
    "    input_tokens = tokens_data.tokens()\n",
    "    offsets = tokens_data[\"offset_mapping\"]\n",
    "\n",
    "    aligned_tokens = []\n",
    "    aligned_labels = []\n",
    "\n",
    "    for token, (tok_start, tok_end) in zip(input_tokens, offsets):\n",
    "        if token in [\"[CLS]\", \"[SEP]\"] or tok_start == tok_end:\n",
    "            continue  # Skip special and null-offset tokens\n",
    "\n",
    "        label = \"O\"\n",
    "        for ent_start, ent_end, ent_label in entities:\n",
    "            if tok_start == ent_start and tok_end <= ent_end:\n",
    "                label = \"B-\" + ent_label\n",
    "                break\n",
    "            elif tok_start > ent_start and tok_end <= ent_end:\n",
    "                label = \"I-\" + ent_label\n",
    "                break\n",
    "\n",
    "        aligned_tokens.append(token)\n",
    "        aligned_labels.append(label)\n",
    "\n",
    "    return aligned_tokens, aligned_labels\n",
    "def clean_annotations(annotations, labels_to_remove=[\"Years of Experience\"]):\n",
    "    cleaned = []\n",
    "    for ann in annotations:\n",
    "        if ann[\"label\"] and ann[\"label\"][0] not in labels_to_remove:\n",
    "            cleaned.append(ann)\n",
    "    return cleaned\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72be6140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A               â†’ B-Name\n",
      "##b             â†’ I-Name\n",
      "##his           â†’ I-Name\n",
      "##he            â†’ I-Name\n",
      "##k             â†’ I-Name\n",
      "J               â†’ I-Name\n",
      "##ha            â†’ O\n",
      "Application     â†’ B-Designation\n",
      "Development     â†’ I-Designation\n",
      "Associate       â†’ O\n",
      "-               â†’ O\n",
      "A               â†’ B-Companies worked at\n",
      "##cc            â†’ I-Companies worked at\n",
      "##ent           â†’ I-Companies worked at\n",
      "##ure           â†’ O\n",
      "Bengal          â†’ B-Location\n",
      "##uru           â†’ O\n",
      ",               â†’ O\n",
      "Karnataka       â†’ O\n",
      "-               â†’ O\n",
      "Em              â†’ O\n",
      "##ail           â†’ O\n",
      "me              â†’ O\n",
      "on              â†’ O\n",
      "Indeed          â†’ B-Email Address\n",
      ":               â†’ I-Email Address\n",
      "indeed          â†’ I-Email Address\n",
      ".               â†’ I-Email Address\n",
      "com             â†’ I-Email Address\n",
      "/               â†’ I-Email Address\n"
     ]
    }
   ],
   "source": [
    "resume_text = data[0][\"content\"]\n",
    "annotations = clean_annotations(data[0].get(\"annotation\", []))\n",
    "aligned_tokens, aligned_labels = align_tokens_with_labels(resume_text, annotations, tokenizer)\n",
    "\n",
    "# Optional: print first few aligned pairs\n",
    "for token, label in zip(aligned_tokens[:30], aligned_labels[:30]):\n",
    "    print(f\"{token:15} â†’ {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0442baa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned resumes: 220\n",
      "Example tokens: ['A', '##b', '##his', '##he', '##k', 'J', '##ha', 'Application', 'Development', 'Associate']\n",
      "Example labels: ['B-Name', 'I-Name', 'I-Name', 'I-Name', 'I-Name', 'I-Name', 'O', 'B-Designation', 'I-Designation', 'O']\n"
     ]
    }
   ],
   "source": [
    "all_tokens = []\n",
    "all_labels = []\n",
    "\n",
    "for item in data:\n",
    "    text = item[\"content\"]\n",
    "    annotations = clean_annotations(item.get(\"annotation\", []))\n",
    "    tokens, labels = align_tokens_with_labels(text, annotations, tokenizer)\n",
    "    all_tokens.append(tokens)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "print(\"Aligned resumes:\", len(all_tokens))\n",
    "print(\"Example tokens:\", all_tokens[0][:10])\n",
    "print(\"Example labels:\", all_labels[0][:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9693509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label to ID mapping: {'B-College Name': 0, 'B-Companies worked at': 1, 'B-Degree': 2, 'B-Designation': 3, 'B-Email Address': 4, 'B-Graduation Year': 5, 'B-Location': 6, 'B-Name': 7, 'B-Skills': 8, 'I-College Name': 9, 'I-Companies worked at': 10, 'I-Degree': 11, 'I-Designation': 12, 'I-Email Address': 13, 'I-Graduation Year': 14, 'I-Location': 15, 'I-Name': 16, 'I-Skills': 17, 'O': 18}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Get unique labels from your aligned labels\n",
    "all_unique_labels = set(label for seq in all_labels for label in seq)\n",
    "label2id = {label: idx for idx, label in enumerate(sorted(all_unique_labels))}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "print(\"Label to ID mapping:\", label2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1820431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 220\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 128\n",
    "input_ids_list = []\n",
    "attention_masks_list = []\n",
    "label_ids_list = []\n",
    "\n",
    "for tokens, labels in zip(all_tokens, all_labels):\n",
    "    # Convert tokens to input IDs\n",
    "    encoding = tokenizer(tokens, is_split_into_words=True, padding='max_length',\n",
    "                         truncation=True, max_length=MAX_LEN, return_tensors='pt')\n",
    "\n",
    "    # Convert labels to IDs and pad\n",
    "    label_ids = [label2id[label] for label in labels]\n",
    "    label_ids = label_ids[:MAX_LEN] + [label2id['O']] * (MAX_LEN - len(label_ids))\n",
    "\n",
    "    input_ids_list.append(encoding['input_ids'][0])\n",
    "    attention_masks_list.append(encoding['attention_mask'][0])\n",
    "    label_ids_list.append(torch.tensor(label_ids))\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "dataset = TensorDataset(\n",
    "    torch.stack(input_ids_list),\n",
    "    torch.stack(attention_masks_list),\n",
    "    torch.stack(label_ids_list)\n",
    ")\n",
    "\n",
    "print(\"Dataset size:\", len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13266e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='88' max='88' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [88/88 02:06, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.854800</td>\n",
       "      <td>0.723421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.704500</td>\n",
       "      <td>0.661600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.665300</td>\n",
       "      <td>0.637141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.617200</td>\n",
       "      <td>0.629670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=88, training_loss=0.7655076384544373, metrics={'train_runtime': 129.5204, 'train_samples_per_second': 5.435, 'train_steps_per_second': 0.679, 'total_flos': 45995297390592.0, 'train_loss': 0.7655076384544373, 'epoch': 4.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert to list of dicts\n",
    "dataset_dicts = []\n",
    "for input_ids, attention_mask, label_ids in zip(input_ids_list, attention_masks_list, label_ids_list):\n",
    "    dataset_dicts.append({\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": label_ids\n",
    "    })\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "hf_dataset = Dataset.from_list(dataset_dicts)\n",
    "hf_dataset = hf_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = hf_dataset[\"train\"]\n",
    "eval_dataset = hf_dataset[\"test\"]\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ff13294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ner_resume_model\\\\tokenizer_config.json',\n",
       " 'ner_resume_model\\\\special_tokens_map.json',\n",
       " 'ner_resume_model\\\\vocab.txt',\n",
       " 'ner_resume_model\\\\added_tokens.json',\n",
       " 'ner_resume_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"ner_resume_model\")\n",
    "tokenizer.save_pretrained(\"ner_resume_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "309ab285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification, AutoTokenizer\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(\"ner_resume_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ner_resume_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5d64a1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, BertForTokenClassification\n",
    "\n",
    "def predict_entities(text, model, tokenizer, label_map):\n",
    "    # Tokenize\n",
    "    tokens_data = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, return_offsets_mapping=True)\n",
    "    input_ids = tokens_data[\"input_ids\"]\n",
    "    attention_mask = tokens_data[\"attention_mask\"]\n",
    "    offsets = tokens_data[\"offset_mapping\"][0]\n",
    "\n",
    "    # Predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    predictions = torch.argmax(outputs.logits, dim=2)[0].tolist()\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    predicted_labels = [label_map[pred] for pred in predictions]\n",
    "\n",
    "    # Extract named entities using BIO tags\n",
    "    entities = []\n",
    "    current_entity = {\"label\": None, \"text\": \"\"}\n",
    "    for token, label, (start, end) in zip(tokens, predicted_labels, offsets):\n",
    "        if token in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
    "            continue\n",
    "        if label.startswith(\"B-\"):\n",
    "            if current_entity[\"text\"]:\n",
    "                entities.append(current_entity)\n",
    "            current_entity = {\"label\": label[2:], \"text\": text[start:end]}\n",
    "        elif label.startswith(\"I-\") and current_entity[\"label\"] == label[2:]:\n",
    "            current_entity[\"text\"] += \" \" + text[start:end]\n",
    "        else:\n",
    "            if current_entity[\"text\"]:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = {\"label\": None, \"text\": \"\"}\n",
    "    if current_entity[\"text\"]:\n",
    "        entities.append(current_entity)\n",
    "    \n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bae58a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = model.config.id2label  # Already maps like {0: 'B-Name', 1: 'I-Name', ...}\n",
    "\n",
    "\n",
    "sample_resume = \"Abhishek Jha Application Development Associate - Accenture Bangalore, Karnataka\"\n",
    "entities = predict_entities(sample_resume, model, tokenizer, label_map)\n",
    "\n",
    "for ent in entities:\n",
    "    print(f\"{ent['label']}: {ent['text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2a4b6db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Tokens: ['P', '##rade', '##ep', 'Kumar', 'Security', 'Ana', '##ly', '##st', 'in', 'In', '##fo', '##sy', '##s', '-', 'Career', 'Con', '##tour', 'Hyderabad', ',', 'Telangana', ',', 'Telangana', '-', 'Em', '##ail', 'me', 'on', 'Indeed', ':', 'indeed']\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted labels:\", predicted_labels[:30])\n",
    "print(\"Tokens:\", tokens[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b89c267d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from training data:\n",
      "\n",
      "Abhishek Jha\n",
      "Application Development Associate - Accenture\n",
      "\n",
      "Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a\n",
      "\n",
      "â€¢ To work for an organization which provides me the opportunity to improve my skills\n",
      "and knowledge for my individual and company's growth in best possibl\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Print part of the first training resume\n",
    "print(\"Sample from training data:\\n\")\n",
    "print(data[0]['content'][:300])  # Preview the first 300 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a0950bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Entities:\n"
     ]
    }
   ],
   "source": [
    "sample_resume = \"\"\"\n",
    "Abhishek Jha\n",
    "Application Development Associate - Accenture\n",
    "\n",
    "Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a\n",
    "\n",
    "â€¢ To work for an organization which provides me the opportunity to improve my skills\n",
    "and knowledge for my individual and company's growth in best possibl\n",
    "\"\"\"\n",
    "\n",
    "entities = predict_entities(sample_resume, model, tokenizer, label_map)\n",
    "\n",
    "# Show predicted entities\n",
    "print(\"\\nPredicted Entities:\")\n",
    "for ent in entities:\n",
    "    print(f\"{ent['label']}: {ent['text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "921e4d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='88' max='88' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [88/88 02:38, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.594600</td>\n",
       "      <td>0.617099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.554400</td>\n",
       "      <td>0.645837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.524500</td>\n",
       "      <td>0.652991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.498800</td>\n",
       "      <td>0.650870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=88, training_loss=0.5347282967784188, metrics={'train_runtime': 160.4383, 'train_samples_per_second': 4.388, 'train_steps_per_second': 0.548, 'total_flos': 45995297390592.0, 'train_loss': 0.5347282967784188, 'epoch': 4.0})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()  # just rerun this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "48123806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ner_resume_model_final\\\\tokenizer_config.json',\n",
       " 'ner_resume_model_final\\\\special_tokens_map.json',\n",
       " 'ner_resume_model_final\\\\vocab.txt',\n",
       " 'ner_resume_model_final\\\\added_tokens.json',\n",
       " 'ner_resume_model_final\\\\tokenizer.json')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"ner_resume_model_final\")\n",
    "tokenizer.save_pretrained(\"ner_resume_model_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "49bfc053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Entities:\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification, AutoTokenizer\n",
    "\n",
    "# Load your retrained model and tokenizer\n",
    "model = BertForTokenClassification.from_pretrained(\"ner_resume_model_final\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ner_resume_model_final\")\n",
    "\n",
    "# Get label mapping\n",
    "label_map = model.config.id2label\n",
    "\n",
    "# Resume sample for testing\n",
    "sample_resume = \"\"\"\n",
    "Abhishek Jha\n",
    "Application Development Associate - Accenture\n",
    "\n",
    "Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a\n",
    "\"\"\"\n",
    "\n",
    "# Predict entities\n",
    "entities = predict_entities(sample_resume, model, tokenizer, label_map)\n",
    "\n",
    "# Show results\n",
    "print(\"\\nPredicted Entities:\")\n",
    "for ent in entities:\n",
    "    print(f\"{ent['label']}: {ent['text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cb051edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution in your training set:\n",
      "B-Name              : 221\n",
      "I-Name              : 721\n",
      "O                   : 81853\n",
      "B-Designation       : 390\n",
      "I-Designation       : 1121\n",
      "B-Companies worked at: 293\n",
      "I-Companies worked at: 834\n",
      "B-Location          : 207\n",
      "B-Email Address     : 202\n",
      "I-Email Address     : 5642\n",
      "B-College Name      : 156\n",
      "I-College Name      : 784\n",
      "B-Graduation Year   : 12\n",
      "B-Skills            : 151\n",
      "I-Skills            : 4863\n",
      "B-Degree            : 132\n",
      "I-Degree            : 521\n",
      "I-Location          : 274\n",
      "I-Graduation Year   : 5\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "flat_labels = [label for sublist in all_labels for label in sublist]\n",
    "label_counts = Counter(flat_labels)\n",
    "\n",
    "print(\"Label distribution in your training set:\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"{label:20}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac2a97fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before balancing: 220 samples\n",
      "After balancing:  220 samples\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "balanced_tokens = []\n",
    "balanced_labels = []\n",
    "\n",
    "for tokens, labels in zip(all_tokens, all_labels):\n",
    "    new_tokens = []\n",
    "    new_labels = []\n",
    "\n",
    "    for token, label in zip(tokens, labels):\n",
    "        if label != \"O\":\n",
    "            new_tokens.append(token)\n",
    "            new_labels.append(label)\n",
    "        else:\n",
    "            # Keep only 20% of 'O' tokens randomly\n",
    "            if random.random() < 0.2:\n",
    "                new_tokens.append(token)\n",
    "                new_labels.append(label)\n",
    "\n",
    "    if new_tokens:  # skip empty examples\n",
    "        balanced_tokens.append(new_tokens)\n",
    "        balanced_labels.append(new_labels)\n",
    "\n",
    "print(f\"Before balancing: {len(all_tokens)} samples\")\n",
    "print(f\"After balancing:  {len(balanced_tokens)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "18db2914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Softwares\\anaconda\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='132' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [132/132 04:09, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.119400</td>\n",
       "      <td>1.037767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.060900</td>\n",
       "      <td>0.953830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.970900</td>\n",
       "      <td>0.962109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.999167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.892900</td>\n",
       "      <td>0.993549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.754700</td>\n",
       "      <td>1.015952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_tokens = balanced_tokens\n",
    "all_labels = balanced_labels\n",
    "input_ids_list = []\n",
    "attention_masks_list = []\n",
    "label_ids_list = []\n",
    "\n",
    "for tokens, labels in zip(all_tokens, all_labels):\n",
    "    encoding = tokenizer(tokens, is_split_into_words=True, padding='max_length',\n",
    "                         truncation=True, max_length=128, return_tensors='pt')\n",
    "    label_ids = [label2id[label] for label in labels]\n",
    "    label_ids = label_ids[:128] + [label2id['O']] * (128 - len(label_ids))\n",
    "\n",
    "    input_ids_list.append(encoding['input_ids'][0])\n",
    "    attention_masks_list.append(encoding['attention_mask'][0])\n",
    "    label_ids_list.append(torch.tensor(label_ids))\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset_dicts = []\n",
    "for input_ids, attention_mask, label_ids in zip(input_ids_list, attention_masks_list, label_ids_list):\n",
    "    dataset_dicts.append({\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": label_ids\n",
    "    })\n",
    "\n",
    "hf_dataset = Dataset.from_list(dataset_dicts)\n",
    "hf_dataset = hf_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = hf_dataset[\"train\"]\n",
    "eval_dataset = hf_dataset[\"test\"]\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_resume_model_balanced\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"ner_resume_model_balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d523469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Entities:\n"
     ]
    }
   ],
   "source": [
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(\"ner_resume_model_balanced\")\n",
    "from transformers import BertForTokenClassification, AutoTokenizer\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(\"ner_resume_model_balanced\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ner_resume_model_balanced\")\n",
    "label_map = model.config.id2label\n",
    "sample_resume = \"\"\"\n",
    "Abhishek Jha\n",
    "Application Development Associate - Accenture\n",
    "\n",
    "Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a\n",
    "\"\"\"\n",
    "\n",
    "entities = predict_entities(sample_resume, model, tokenizer, label_map)\n",
    "\n",
    "print(\"\\nPredicted Entities:\")\n",
    "for ent in entities:\n",
    "    print(f\"{ent['label']}: {ent['text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a538f181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEBUG: Showing first 30 predicted labels\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['P', '##rade', '##ep', 'Kumar', 'Security', 'Ana', '##ly', 'In', '##fo', '##sy', '##s', '-', 'Career', 'Con', ',', '/', '##ep', 'Kumar', '/', '##55', '##d', '##9', '##8', 'Security', 'Ana', '##ly', '##tour', 'Security', 'is', ',']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDEBUG: Showing first 30 predicted labels\")\n",
    "print(predicted_labels[:30])\n",
    "print(tokens[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "67823abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacyNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached spacy-3.8.7-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\softwares\\anaconda\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\softwares\\anaconda\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\softwares\\anaconda\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\softwares\\anaconda\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\softwares\\anaconda\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Using cached thinc-8.3.6-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\softwares\\anaconda\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\softwares\\anaconda\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\softwares\\anaconda\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\softwares\\anaconda\\lib\\site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\softwares\\anaconda\\lib\\site-packages (from spacy) (0.15.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\softwares\\anaconda\\lib\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\softwares\\anaconda\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\softwares\\anaconda\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\softwares\\anaconda\\lib\\site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\softwares\\anaconda\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\softwares\\anaconda\\lib\\site-packages (from spacy) (76.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\softwares\\anaconda\\lib\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\softwares\\anaconda\\lib\\site-packages (from spacy) (3.4.1)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\softwares\\anaconda\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\softwares\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\softwares\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\softwares\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\softwares\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\softwares\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\softwares\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\softwares\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
      "Requirement already satisfied: cloudpickle>=2.2.0 in c:\\softwares\\anaconda\\lib\\site-packages (from srsly<3.0.0,>=2.4.3->spacy) (3.0.0)\n",
      "Requirement already satisfied: ujson>=1.35 in c:\\softwares\\anaconda\\lib\\site-packages (from srsly<3.0.0,>=2.4.3->spacy) (5.10.0)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Using cached blis-1.3.0-cp312-cp312-win_amd64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\softwares\\anaconda\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Using cached numpy-2.2.6-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: colorama in c:\\softwares\\anaconda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<8.2,>=8.0.0 in c:\\softwares\\anaconda\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\softwares\\anaconda\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\softwares\\anaconda\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.9.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\softwares\\anaconda\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\softwares\\anaconda\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\softwares\\anaconda\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\softwares\\anaconda\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\softwares\\anaconda\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\softwares\\anaconda\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\softwares\\anaconda\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Using cached spacy-3.8.7-cp312-cp312-win_amd64.whl (13.9 MB)\n",
      "Using cached thinc-8.3.6-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "Using cached blis-1.3.0-cp312-cp312-win_amd64.whl (6.3 MB)\n",
      "Using cached numpy-2.2.6-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "Downloading typer-0.9.4-py3-none-any.whl (45 kB)\n",
      "Installing collected packages: numpy, typer, blis, thinc, spacy\n",
      "\n",
      "  Attempting uninstall: numpy\n",
      "\n",
      "    Found existing installation: numpy 1.26.4\n",
      "\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "    Uninstalling numpy-1.26.4:\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "  Attempting uninstall: typer\n",
      "   ---------------------------------------- 0/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [typer]\n",
      "    Found existing installation: typer 0.15.4\n",
      "   -------- ------------------------------- 1/5 [typer]\n",
      "    Uninstalling typer-0.15.4:\n",
      "   -------- ------------------------------- 1/5 [typer]\n",
      "      Successfully uninstalled typer-0.15.4\n",
      "   -------- ------------------------------- 1/5 [typer]\n",
      "  Attempting uninstall: blis\n",
      "   -------- ------------------------------- 1/5 [typer]\n",
      "    Found existing installation: blis 0.7.11\n",
      "   -------- ------------------------------- 1/5 [typer]\n",
      "    Uninstalling blis-0.7.11:\n",
      "   -------- ------------------------------- 1/5 [typer]\n",
      "   ---------------- ----------------------- 2/5 [blis]\n",
      "      Successfully uninstalled blis-0.7.11\n",
      "   ---------------- ----------------------- 2/5 [blis]\n",
      "   ---------------- ----------------------- 2/5 [blis]\n",
      "  Attempting uninstall: thinc\n",
      "   ---------------- ----------------------- 2/5 [blis]\n",
      "    Found existing installation: thinc 8.2.5\n",
      "   ---------------- ----------------------- 2/5 [blis]\n",
      "    Uninstalling thinc-8.2.5:\n",
      "   ---------------- ----------------------- 2/5 [blis]\n",
      "      Successfully uninstalled thinc-8.2.5\n",
      "   ---------------- ----------------------- 2/5 [blis]\n",
      "   ------------------------ --------------- 3/5 [thinc]\n",
      "   ------------------------ --------------- 3/5 [thinc]\n",
      "   ------------------------ --------------- 3/5 [thinc]\n",
      "   ------------------------ --------------- 3/5 [thinc]\n",
      "   -------------------------------- ------- 4/5 [spacy]\n",
      "   -------------------------------- ------- 4/5 [spacy]\n",
      "   -------------------------------- ------- 4/5 [spacy]\n",
      "   -------------------------------- ------- 4/5 [spacy]\n",
      "   -------------------------------- ------- 4/5 [spacy]\n",
      "   -------------------------------- ------- 4/5 [spacy]\n",
      "   -------------------------------- ------- 4/5 [spacy]\n",
      "   -------------------------------- ------- 4/5 [spacy]\n",
      "   -------------------------------- ------- 4/5 [spacy]\n",
      "   -------------------------------- ------- 4/5 [spacy]\n",
      "   -------------------------------- ------- 4/5 [spacy]\n",
      "   -------------------------------- ------- 4/5 [spacy]\n",
      "   -------------------------------- ------- 4/5 [spacy]\n",
      "   -------------------------------- ------- 4/5 [spacy]\n",
      "   -------------------------------- ------- 4/5 [spacy]\n",
      "   -------------------------------- ------- 4/5 [spacy]\n",
      "   -------------------------------- ------- 4/5 [spacy]\n",
      "   -------------------------------- ------- 4/5 [spacy]\n",
      "   -------------------------------- ------- 4/5 [spacy]\n",
      "   ---------------------------------------- 5/5 [spacy]\n",
      "\n",
      "Successfully installed blis-1.0.1 numpy-2.2.6 spacy-3.8.7 thinc-8.3.2 typer-0.9.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.2.6 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… spaCy is working perfectly now!\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"âœ… spaCy is working perfectly now!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7369f84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SpaCy is finally working!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"âœ… SpaCy is finally working!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0d05cf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Abhishek Jha\n",
      "Designation: Application Development Associate\n",
      "Company: Accenture\n",
      "Location: Bengaluru\n",
      "Email: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "# Load model\n",
    "nlp = spacy.load(\"en_core_web_sm\",exclude=[\"ner\"])\n",
    "# Add EntityRuler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "# Define your patterns\n",
    "patterns = [\n",
    "    {\"label\": \"Name\", \"pattern\": [{\"LOWER\": \"abhishek\"}, {\"LOWER\": \"jha\"}]},\n",
    "    {\"label\": \"Designation\", \"pattern\": \"Application Development Associate\"},\n",
    "    {\"label\": \"Company\", \"pattern\": \"Accenture\"},\n",
    "    {\"label\": \"Location\", \"pattern\": \"Bengaluru\"},\n",
    "    {\"label\": \"Email\", \"pattern\": [{\"TEXT\": {\"REGEX\": r\"indeed\\.com/.+\"}}]}\n",
    "]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# Sample test\n",
    "doc = nlp(\"\"\"\n",
    "Abhishek Jha\n",
    "Application Development Associate - Accenture\n",
    "Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a\n",
    "\"\"\")\n",
    "\n",
    "# Print results\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.label_}: {ent.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e1fcf239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Softwares\\anaconda\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Name       1.00      1.00      1.00         1\n",
      "      Skills       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       1.00      0.50      0.67         2\n",
      "   macro avg       0.50      0.50      0.50         2\n",
      "weighted avg       0.50      0.50      0.50         2\n",
      "\n",
      "F1 Score: 0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Softwares\\anaconda\\Lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Abhishek Jha works at Accenture.\" with entities \"[(0, 13, 'Name'), (23, 32, 'Company')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'token_acc': 1.0,\n",
       " 'token_p': 1.0,\n",
       " 'token_r': 1.0,\n",
       " 'token_f': 1.0,\n",
       " 'sents_p': None,\n",
       " 'sents_r': None,\n",
       " 'sents_f': None,\n",
       " 'tag_acc': None,\n",
       " 'pos_acc': None,\n",
       " 'morph_acc': None,\n",
       " 'morph_micro_p': None,\n",
       " 'morph_micro_r': None,\n",
       " 'morph_micro_f': None,\n",
       " 'morph_per_feat': None,\n",
       " 'dep_uas': None,\n",
       " 'dep_las': None,\n",
       " 'dep_las_per_type': None,\n",
       " 'ents_p': None,\n",
       " 'ents_r': None,\n",
       " 'ents_f': None,\n",
       " 'ents_per_type': None,\n",
       " 'cats_score': 0.0,\n",
       " 'cats_score_desc': 'macro F',\n",
       " 'cats_micro_p': 0.0,\n",
       " 'cats_micro_r': 0.0,\n",
       " 'cats_micro_f': 0.0,\n",
       " 'cats_macro_p': 0.0,\n",
       " 'cats_macro_r': 0.0,\n",
       " 'cats_macro_f': 0.0,\n",
       " 'cats_macro_auc': 0.0,\n",
       " 'cats_f_per_type': {},\n",
       " 'cats_auc_per_type': {}}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report, f1_score\n",
    "\n",
    "y_true = [['B-Name', 'I-Name', 'O', 'B-Skills', 'O']]\n",
    "y_pred = [['B-Name', 'I-Name', 'O', 'O', 'O']]\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_true, y_pred))\n",
    "import spacy\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.training import Example\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # or your rule-based model\n",
    "\n",
    "# Define gold data\n",
    "example = Example.from_dict(\n",
    "    nlp.make_doc(\"Abhishek Jha works at Accenture.\"),\n",
    "    {\"entities\": [(0, 13, \"Name\"), (23, 32, \"Company\")]}\n",
    ")\n",
    "\n",
    "scorer = Scorer()\n",
    "scorer.score([example])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57102b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
